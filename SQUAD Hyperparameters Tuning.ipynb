{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0222cb62-8f6d-48a8-b0e4-5d04400ee154",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for NLP Models\n",
    "\n",
    "This notebook focuses on the hyperparameter tuning of five different transformer-based models to optimize their performance for question answering task.\n",
    "\n",
    "## Models Used\n",
    "- **RoBERTa**: A robustly optimized BERT approach that modifies key hyperparameters in BERT and removes the next-sentence pretraining objective.\n",
    "- **DistilBERT**: A distilled version of BERT that retains most of the performance of BERT but with fewer parameters and faster inference time.\n",
    "- **XLM-RoBERTa**: A multilingual model trained on 100 different languages, suitable for cross-lingual transfer learning.\n",
    "\n",
    "## Objectives\n",
    "- Identify the best hyperparameters for each model based on F1-score and exact matching.\n",
    "- Re-tune the models for multiclass classification on bias type.\n",
    "- Compare the performance across models and classification tasks to determine the most effective model configuration.\n",
    "\n",
    "Each section of the notebook will guide you through the process of data preparation, model configuration, hyperparameter tuning, and evaluation of results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b34d3-ce01-4928-8231-ceb2b8b37550",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Imports, libraries and rusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29728d32-b961-4529-aa97-b25c37f9cc55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import ast\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import random\n",
    "import collections\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Optional\n",
    "from IPython.display import HTML, display\n",
    "import math\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "# Data Handling Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from torch.utils.data import random_split\n",
    "import datasets\n",
    "from datasets import ClassLabel, Sequence\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import scikitplot as skplt  # Uncomment if scikit-plot is installed and needed\n",
    "\n",
    "# Machine Learning: Model Preparation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Machine Learning: Models and Frameworks\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import evaluate\n",
    "import xgboost\n",
    "import wandb\n",
    "from xgboost import plot_importance  # Uncomment if xgboost importance plot is required\n",
    "\n",
    "\n",
    "# NLP and Transformers\n",
    "from transformers import (AdamW, AutoModelForSequenceClassification, AutoModelForQuestionAnswering,\n",
    "                          AutoTokenizer, CamembertForSequenceClassification, DistilBertConfig,\n",
    "                          DistilBertForSequenceClassification, DistilBertModel, EarlyStoppingCallback,\n",
    "                          get_linear_schedule_with_warmup, RobertaForSequenceClassification, EvalPrediction,\n",
    "                          Trainer, TrainerCallback, TrainingArguments, XLMRobertaForSequenceClassification,\n",
    "                         DefaultDataCollator, BertForQuestionAnswering, DataCollatorWithPadding, PreTrainedTokenizerFast,\n",
    "                         default_data_collator, is_torch_xla_available)\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from transformers.trainer_utils import PredictionOutput, speed_metrics\n",
    "\n",
    "# Experiment Tracking and Optimization Utilities\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "# import wandb  # Uncomment if using Weights & Biases for experiment tracking\n",
    "\n",
    "# Progress Bar Utilities\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e89523a9-11e8-449d-90b3-cd754ad8d672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb.login(key='8f7092f0fdaf14add2b4cc07cb0e740080cdd8e7')\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11eb8e8c-404c-4214-9dab-c32ce8b8a415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 Ti SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")\n",
    "\n",
    "### Compute_metrics function for Question and Answering problem is different to classification, more preocessing required.\n",
    "\n",
    "metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "        return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fb0920-275c-461a-b378-2e6c1148d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A subclass of `Trainer` specific to Question-Answering tasks\n",
    "\"\"\"\n",
    "\n",
    "if is_torch_xla_available():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.debug.metrics as met\n",
    "\n",
    "\n",
    "class QuestionAnsweringTrainer(Trainer):\n",
    "    def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_examples = eval_examples\n",
    "        self.post_process_function = post_process_function\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
    "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        eval_examples = self.eval_examples if eval_examples is None else eval_examples\n",
    "\n",
    "        # Temporarily disable metric computation, we will do it in the loop here.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            output = eval_loop(\n",
    "                eval_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "                metric_key_prefix=metric_key_prefix,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=output.num_samples,\n",
    "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "        if self.post_process_function is not None and self.compute_metrics is not None and self.args.should_save:\n",
    "            # Only the main node write the results by default\n",
    "            eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)\n",
    "            metrics = self.compute_metrics(eval_preds)\n",
    "\n",
    "            # Prefix all keys with metric_key_prefix + '_'\n",
    "            for key in list(metrics.keys()):\n",
    "                if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                    metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "            metrics.update(output.metrics)\n",
    "        else:\n",
    "            metrics = output.metrics\n",
    "\n",
    "        if self.args.should_log:\n",
    "            # Only the main node log the results by default\n",
    "            self.log(metrics)\n",
    "\n",
    "        if self.args.tpu_metrics_debug or self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, predict_dataset, predict_examples, ignore_keys=None, metric_key_prefix: str = \"test\"):\n",
    "        predict_dataloader = self.get_test_dataloader(predict_dataset)\n",
    "\n",
    "        # Temporarily disable metric computation, we will do it in the loop here.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            output = eval_loop(\n",
    "                predict_dataloader,\n",
    "                description=\"Prediction\",\n",
    "                # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "                metric_key_prefix=metric_key_prefix,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=output.num_samples,\n",
    "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.post_process_function is None or self.compute_metrics is None:\n",
    "            return output\n",
    "\n",
    "        predictions = self.post_process_function(predict_examples, predict_dataset, output.predictions, \"predict\")\n",
    "        metrics = self.compute_metrics(predictions)\n",
    "\n",
    "        # Prefix all keys with metric_key_prefix + '_'\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "        metrics.update(output.metrics)\n",
    "        return PredictionOutput(predictions=predictions.predictions, label_ids=predictions.label_ids, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b34e9dc1-13cf-41ac-ae24-0a12570cdc87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 1000\n",
    "global global_doc_stride\n",
    "#global_doc_stride = 128\n",
    "pretrained_model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "\n",
    "## The dataset does not have the ending index of the \"Answers\" which is required for the Question and Answering transformer.\n",
    "## This function uses the data in the Squad dataset to extract the start index and end index as well as tokenizes the dataset.\n",
    "## after running this function, the data is ready to be used in the model\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\" if right_padding else \"only_first\",\n",
    "        stride=global_doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        if len(answer[\"answer_start\"]) > 0:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        else:\n",
    "            start_char = -1\n",
    "            end_char = -1\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx -1\n",
    "        \n",
    "\n",
    "        # No answer\n",
    "        if start_char == -1 and end_char == -1:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        elif offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 2)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "557b3dcb-f6ba-4d78-9d90-291f52ed4fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "  # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "  # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "  # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "  # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "  # in one example possible giving several features when a context is long, each of those features having a\n",
    "  # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "      examples[\"question\"],\n",
    "      examples[\"context\"],\n",
    "      truncation=\"only_second\" if right_padding else \"only_first\",\n",
    "      max_length=max_length,\n",
    "      stride=global_doc_stride,\n",
    "      return_overflowing_tokens=True,\n",
    "      return_offsets_mapping=True,\n",
    "      padding=\"max_length\",\n",
    "  )\n",
    "\n",
    "  # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "  # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "  # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "  # corresponding example_id and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "      # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 0\n",
    "\n",
    "      # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "      # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "      # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "      ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f8d9591-1814-41e3-9a5c-6fccb4486a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def postprocess_qa_predictions(\n",
    "    examples,\n",
    "    features,\n",
    "    predictions: Tuple[np.ndarray, np.ndarray],\n",
    "    version_2_with_negative: bool = True,\n",
    "    n_best_size: int = 20,\n",
    "    max_answer_length: int = 30,\n",
    "    null_score_diff_threshold: float = 0.0,\n",
    "    output_dir: Optional[str] = None,\n",
    "    prefix: Optional[str] = None,\n",
    "    log_level: Optional[int] = logging.WARNING,\n",
    "):\n",
    "    \"\"\"\n",
    "    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n",
    "    original contexts. This is the base postprocessing functions for models that only return start and end logits.\n",
    "\n",
    "    Args:\n",
    "        examples: The non-preprocessed dataset (see the main script for more information).\n",
    "        features: The processed dataset (see the main script for more information).\n",
    "        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n",
    "            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n",
    "            first dimension must match the number of elements of :obj:`features`.\n",
    "        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether or not the underlying dataset contains examples with no answers.\n",
    "        n_best_size (:obj:`int`, `optional`, defaults to 20):\n",
    "            The total number of n-best predictions to generate when looking for an answer.\n",
    "        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n",
    "            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n",
    "            are not conditioned on one another.\n",
    "        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\n",
    "            The threshold used to select the null answer: if the best answer has a score that is less than the score of\n",
    "            the null answer minus this threshold, the null answer is selected for this example (note that the score of\n",
    "            the null answer for an example giving several features is the minimum of the scores for the null answer on\n",
    "            each feature: all features must be aligned on the fact they `want` to predict a null answer).\n",
    "\n",
    "            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\n",
    "        output_dir (:obj:`str`, `optional`):\n",
    "            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\n",
    "            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\n",
    "            answers, are saved in `output_dir`.\n",
    "        prefix (:obj:`str`, `optional`):\n",
    "            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\n",
    "        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\n",
    "            ``logging`` log level (e.g., ``logging.WARNING``)\n",
    "    \"\"\"\n",
    "    if len(predictions) != 2:\n",
    "        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n",
    "    all_start_logits, all_end_logits = predictions\n",
    "\n",
    "    if len(predictions[0]) != len(features):\n",
    "        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n",
    "\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    if version_2_with_negative:\n",
    "        scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    logger.setLevel(log_level)\n",
    "    logger.info(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_prediction = None\n",
    "        prelim_predictions = []\n",
    "\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
    "            # available in the current feature.\n",
    "            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            feature_null_score = start_logits[0] + end_logits[0]\n",
    "            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n",
    "                min_null_prediction = {\n",
    "                    \"offsets\": (0, 0),\n",
    "                    \"score\": feature_null_score,\n",
    "                    \"start_logit\": start_logits[0],\n",
    "                    \"end_logit\": end_logits[0],\n",
    "                }\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or len(offset_mapping[start_index]) < 2\n",
    "                        or offset_mapping[end_index] is None\n",
    "                        or len(offset_mapping[end_index]) < 2\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
    "                    # provided).\n",
    "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
    "                        continue\n",
    "\n",
    "                    prelim_predictions.append(\n",
    "                        {\n",
    "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"start_logit\": start_logits[start_index],\n",
    "                            \"end_logit\": end_logits[end_index],\n",
    "                        }\n",
    "                    )\n",
    "        if version_2_with_negative and min_null_prediction is not None:\n",
    "            # Add the minimum null prediction\n",
    "            prelim_predictions.append(min_null_prediction)\n",
    "            null_score = min_null_prediction[\"score\"]\n",
    "\n",
    "        # Only keep the best `n_best_size` predictions.\n",
    "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "\n",
    "        # Add back the minimum null prediction if it was removed because of its low score.\n",
    "        if (\n",
    "            version_2_with_negative\n",
    "            and min_null_prediction is not None\n",
    "            and not any(p[\"offsets\"] == (0, 0) for p in predictions)\n",
    "        ):\n",
    "            predictions.append(min_null_prediction)\n",
    "\n",
    "        # Use the offsets to gather the answer text in the original context.\n",
    "        context = example[\"context\"]\n",
    "        for pred in predictions:\n",
    "            offsets = pred.pop(\"offsets\")\n",
    "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
    "\n",
    "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "        # failure.\n",
    "        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n",
    "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
    "\n",
    "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
    "        # the LogSumExp trick).\n",
    "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
    "        exp_scores = np.exp(scores - np.max(scores))\n",
    "        probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "        # Include the probabilities in our predictions.\n",
    "        for prob, pred in zip(probs, predictions):\n",
    "            pred[\"probability\"] = prob\n",
    "\n",
    "        # Pick the best prediction. If the null answer is not possible, this is easy.\n",
    "        if not version_2_with_negative:\n",
    "            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n",
    "        else:\n",
    "            # Otherwise we first need to find the best non-empty prediction.\n",
    "            i = 0\n",
    "            while predictions[i][\"text\"] == \"\":\n",
    "                i += 1\n",
    "            best_non_null_pred = predictions[i]\n",
    "\n",
    "            # Then we compare to the null prediction using the threshold.\n",
    "            score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n",
    "            scores_diff_json[example[\"id\"]] = float(score_diff)  # To be JSON-serializable.\n",
    "            if score_diff > null_score_diff_threshold:\n",
    "                all_predictions[example[\"id\"]] = \"\"\n",
    "            else:\n",
    "                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
    "\n",
    "        # Make `predictions` JSON-serializable by casting np.float back to float.\n",
    "        all_nbest_json[example[\"id\"]] = [\n",
    "            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n",
    "            for pred in predictions\n",
    "        ]\n",
    "\n",
    "    # If we have an output_dir, let's save all those dicts.\n",
    "    if output_dir is not None:\n",
    "        if not os.path.isdir(output_dir):\n",
    "            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n",
    "\n",
    "        prediction_file = os.path.join(\n",
    "            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n",
    "        )\n",
    "        nbest_file = os.path.join(\n",
    "            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n",
    "        )\n",
    "        if version_2_with_negative:\n",
    "            null_odds_file = os.path.join(\n",
    "                output_dir, \"null_odds.json\" if prefix is None else f\"{prefix}_null_odds.json\"\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Saving predictions to {prediction_file}.\")\n",
    "        with open(prediction_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n",
    "        with open(nbest_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "        if version_2_with_negative:\n",
    "            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n",
    "            with open(null_odds_file, \"w\") as writer:\n",
    "                writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f38728fd-37fd-415f-8eb6-348beccf5f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "        # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "        predictions = postprocess_qa_predictions(\n",
    "            examples=examples,\n",
    "            features=features,\n",
    "            predictions=predictions,\n",
    "            max_answer_length=max_length\n",
    "        )\n",
    "        # Format the result to the format the metric expects.\n",
    "        if 1==1:\n",
    "            formatted_predictions = [\n",
    "                {\"id\": str(k), \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "            ]\n",
    "        else:\n",
    "            formatted_predictions = [{\"id\": str(k), \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "        references = [{\"id\": str(ex[\"id\"]), \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "        return EvalPrediction(predictions=formatted_predictions, label_ids=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d08dd49a-5910-4732-9870-0c0c9acfbdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdvancedEarlyStoppingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback to stop training when either the performance falls below a certain threshold\n",
    "    or if there is no improvement over a set number of epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, metric_name, patience, threshold):\n",
    "        self.metric_name = metric_name\n",
    "        self.patience = patience\n",
    "        self.threshold = threshold\n",
    "        self.best_score = None\n",
    "        self.no_improve_epochs = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        metric_value = kwargs['metrics'].get(self.metric_name)\n",
    "\n",
    "        if self.best_score is None or metric_value > self.best_score:\n",
    "            self.best_score = metric_value\n",
    "            self.no_improve_epochs = 0\n",
    "        else:\n",
    "            self.no_improve_epochs += 1\n",
    "\n",
    "        # Check if performance is below the threshold\n",
    "        if metric_value < self.threshold:\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Stopping training: {self.metric_name} below threshold of {self.threshold}\")\n",
    "\n",
    "        # Check if no improvement has been seen over the allowed patience\n",
    "        if self.no_improve_epochs >= self.patience:\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Stopping training: No improvement in {self.metric_name} for {self.patience} epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd3fb22a-82f2-493a-afeb-78425296fe87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model initialization function\n",
    "def model_init():\n",
    "    return AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Define train dataset initialization function\n",
    "def train_dataset_init():\n",
    "    return squad_raw['train'].map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                remove_columns=squad_raw[\"train\"].column_names,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )\n",
    "\n",
    "# Define validation dataset initialization function\n",
    "def vald_dataset_init():\n",
    "    return squad_raw['validation'].map(\n",
    "                prepare_validation_features,\n",
    "                batched=True,\n",
    "                remove_columns=squad_raw[\"train\"].column_names,\n",
    "                desc=\"Running tokenizer on validation dataset\",\n",
    "            )\n",
    "\n",
    "# Optuna objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 1e-4, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    warmup_steps = trial.suggest_int('warmup_steps', 0, 1000)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 0.01, 0.25)\n",
    "    adam_beta1 = trial.suggest_float('adam_beta1', 0.8, 0.95)\n",
    "    adam_beta2 = trial.suggest_float('adam_beta2', 0.990, 0.999)\n",
    "    adam_epsilon = trial.suggest_float('adam_epsilon', 1e-8, 1e-6)\n",
    "    lr_scheduler_type = trial.suggest_categorical('lr_scheduler_type', ['linear', 'cosine', 'cosine_with_restarts','constant_with_warmup'])\n",
    "    output_dir = f\"./{normalized_model_name}-finetuned-squadv2/trial_{trial.number}\"\n",
    "    \n",
    "    #global global_doc_stride\n",
    "    #global_doc_stride=trial.suggest_int('doc_stride', 128, 256, step=64)\n",
    "\n",
    "    # Print trial parameters\n",
    "    print(f\"Current Trial {trial.number} parameters: {trial.params}\")\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir = True,\n",
    "        metric_for_best_model='f1',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2, # Save only the best model unless you specify a different number\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=10,  # Adjust based on computation limits\n",
    "        report_to=\"wandb\",  # Enable logging to Weights & Biases        \n",
    "        run_name=f\"{normalized_model_name}-finetune-squadv2\",  # Optionally set a specific run name    \n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=warmup_steps,\n",
    "        weight_decay=weight_decay,\n",
    "        adam_beta1=adam_beta1,\n",
    "        adam_beta2=adam_beta2,\n",
    "        adam_epsilon=adam_epsilon,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        fp16=True,  # Enable mixed-precision training\n",
    "    )    \n",
    "\n",
    "    trainer = QuestionAnsweringTrainer(\n",
    "        model=model_init(),\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_init(),\n",
    "        eval_dataset=vald_dataset_init(),\n",
    "        eval_examples=eval_examples,        \n",
    "        data_collator=data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics,\n",
    "        #callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    "        callbacks=[AdvancedEarlyStoppingCallback(metric_name='eval_f1', patience=1, threshold=45)]\n",
    "    )  \n",
    "    \n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "    #print(\"Evaluation results:\", eval_results)  # Debug print\n",
    "    return eval_results['eval_f1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897e6ef-fa3d-4131-be3d-e09548d282a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4d5f9be3-8e70-453b-8d68-61ce0403e833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "global global_doc_stride\n",
    "#global_doc_stride = 128\n",
    "pretrained_model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "\n",
    "## The dataset does not have the ending index of the \"Answers\" which is required for the Question and Answering transformer.\n",
    "## This function uses the data in the Squad dataset to extract the start index and end index as well as tokenizes the dataset.\n",
    "## after running this function, the data is ready to be used in the model\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        stride=global_doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        if len(answer[\"answer_start\"]) > 0:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        else:\n",
    "            start_char = -1\n",
    "            end_char = -1\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx -1\n",
    "        \n",
    "\n",
    "        # No answer\n",
    "        if start_char == -1 and end_char == -1:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        elif offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 2)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed46ac9-78d4-4d19-8760-dc07373701d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "# Training preprocessing\n",
    "    def prepare_train_features(examples):\n",
    "        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "        # left whitespace\n",
    "        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_length,\n",
    "            stride=data_args.doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "        )\n",
    "\n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            if tokenizer.cls_token_id in input_ids:\n",
    "                cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "            elif tokenizer.bos_token_id in input_ids:\n",
    "                cls_index = input_ids.index(tokenizer.bos_token_id)\n",
    "            else:\n",
    "                cls_index = 0\n",
    "\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[answer_column_name][sample_index]\n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "        return tokenized_examples"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed19e288-b6c4-4486-bb1a-d22ff7dfb616",
   "metadata": {
    "tags": []
   },
   "source": [
    "max_length = 512  # Adjust max_length to fit model constraints\n",
    "global global_doc_stride\n",
    "#global_doc_stride = 128\n",
    "pretrained_model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance(tokenizer, PreTrainedTokenizerFast)\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,  # Updated to handle model max length\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        stride=global_doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        if len(answer[\"answer_start\"]) > 0:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        else:\n",
    "            start_char = -1\n",
    "            end_char = -1\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # Adjust indices for answers that are partially or not within the truncated context\n",
    "        if start_char == -1 and end_char == -1:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        elif offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Locate the token start and end positions within the context\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(max(idx - 1, 0))  # Ensure the position is not negative\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(min(idx + 1, max_length - 1))  # Ensure position does not exceed max_length\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "1693c1f9-d0c7-4abe-aeee-121f2fddb0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 512  # Adjust max_length to fit model constraints\n",
    "global global_doc_stride\n",
    "#global_doc_stride = 128\n",
    "pretrained_model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance(tokenizer, PreTrainedTokenizerFast)\n",
    "pad_on_right = tokenizer.padding_side == 'right'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,  # Updated to handle model max length\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        stride=global_doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        if tokenizer.cls_token_id in input_ids:\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        elif tokenizer.bos_token_id in input_ids:\n",
    "            cls_index = input_ids.index(tokenizer.bos_token_id)\n",
    "        else:\n",
    "            cls_index = 0\n",
    "        \n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        \n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples['answers'][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)                \n",
    "\n",
    "    return tokenized_examples\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaec4a80-9693-4ee8-a106-7fc0b4643640",
   "metadata": {
    "tags": []
   },
   "source": [
    "max_length = 512  # Adjust max_length to fit model constraints\n",
    "global_doc_stride = 128\n",
    "pretrained_model_name = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance(tokenizer, PreTrainedTokenizerFast)\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,  # Updated to handle model max length\n",
    "        truncation=True,  # Adjusted to ensure truncation if the combined length exceeds max_length\n",
    "        stride=global_doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        if len(answer[\"answer_start\"]) > 0:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        else:\n",
    "            start_char = -1\n",
    "            end_char = -1\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        \n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # Adjust indices for answers that are partially or not within the truncated context\n",
    "        if start_char == -1 and end_char == -1:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        elif offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Locate the token start and end positions within the context\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(max(idx - 1, 0))  # Ensure the position is not negative\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(min(idx + 1, max_length - 2))  # Ensure position does not exceed max_length\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cef6a8eb-797e-489f-b1cc-385feb91c004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## IMPORT RAW DATASET\n",
    "\n",
    "squad_dataset = load_dataset(\"squad_v2\", split=\"train[:2000]\")\n",
    "train_testvalid = squad_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "squad_raw = datasets.DatasetDict({\n",
    "                                'train': train_testvalid['train'],\n",
    "                                'validation': test_valid['train'],\n",
    "                                'test': test_valid['test']\n",
    "                                })"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52048c6c-08dc-489b-95de-7c7538c92374",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialize a list to store data\n",
    "data = []\n",
    "\n",
    "# Iterate over each entry in the dataset\n",
    "for index, example in enumerate(squad_dataset):\n",
    "    entry = {\n",
    "        'Index': index,\n",
    "        'ID': example['id'],\n",
    "        'Question': example['question'],\n",
    "        'Answer': example['answers']['text'][0] if example['answers']['text'] else \"No answer\"\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df.tail(50))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1218e56-8294-4762-b319-b87e22e1e57f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the full dataset\n",
    "squad_dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "# Organize into a DatasetDict\n",
    "squad_raw = datasets.DatasetDict({\n",
    "    'train': squad_dataset['train'],\n",
    "    'validation': squad_dataset['validation'],    \n",
    "})\n",
    "\n",
    "# Display the sizes of the splits to confirm\n",
    "print(\"Train set size:\", len(squad_raw['train']))\n",
    "print(\"Validation set size:\", len(squad_raw['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d80a1fb-7498-41d6-bbd3-9638b9f31bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_raw"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77def231-d114-44a3-875b-a282485e5eef",
   "metadata": {
    "tags": []
   },
   "source": [
    "def get_longest_answer_length(dataset):\n",
    "    max_length = 0\n",
    "    longest_answer = None\n",
    "    for example in dataset:\n",
    "        for answer in example['answers']['text']:\n",
    "            if len(answer) > max_length:\n",
    "                max_length = len(answer)\n",
    "                longest_answer = answer\n",
    "    return max_length, longest_answer\n",
    "\n",
    "# Apply the function to the train and validation set\n",
    "longest_train_length, longest_train_answer = get_longest_answer_length(squad_raw['train'])\n",
    "longest_validation_length, longest_validation_answer = get_longest_answer_length(squad_raw['validation'])\n",
    "\n",
    "print(\"Longest Answer in Train Set:\", longest_train_answer, \"Length:\", longest_train_length)\n",
    "print(\"Longest Answer in Validation Set:\", longest_validation_answer, \"Length:\", longest_validation_length)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ed60b8c-a6ea-42e6-9861-6a9dd359044a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example text with approximately 239 characters\n",
    "text = \"that the sudden shift of a huge quantity of water into the region could have relaxed the tension between the two sides of the fault, allowing them to move apart, and could have increased the direct pressure on it, causing a violent rupture\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the tokens and the number of tokens\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Number of tokens:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6369c142-9805-41c3-b937-3cc2df1bd2d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806caee4ae174ef6af26fcb7db91ed35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global_doc_stride = 128\n",
    "global train_dataset\n",
    "global eval_dataset\n",
    "## APPLY PREPROCESSING\n",
    "train_dataset = squad_raw['train'].map(\n",
    "                preprocess_function,\n",
    "                batched=True,\n",
    "                remove_columns=squad_raw[\"train\"].column_names,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )\n",
    "eval_dataset = squad_raw['validation'].map(\n",
    "                prepare_validation_features,\n",
    "                batched=True,\n",
    "                remove_columns=squad_raw[\"train\"].column_names,\n",
    "                desc=\"Running tokenizer on validation dataset\",\n",
    "            )\n",
    "eval_examples =  squad_raw[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "68b6cd0b-3d1b-4658-bf7f-09e0f8b189ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mismatches in train: 1562\n",
      "[{'example_id': '56d39bdf59d6e41400146808', 'decoded_answer': '', 'actual_answer': '1833', 'context': 'The 21 nocturnes are more structured, and of greater emotional depth, than those of Field (whom Chopin met in 1833). Many of the Chopin nocturnes have middle sections marked by agitated expression (and often making very difficult demands on the performer) which heightens their dramatic character.', 'question': 'What year did Chopin meet Field?'}, {'example_id': '56cc84ab6d243a140015efe3', 'decoded_answer': '1833', 'actual_answer': 'individual seat-back displays', 'context': 'Beginning in mid-2007, four major airlines, United, Continental, Delta, and Emirates, reached agreements to install iPod seat connections. The free service will allow passengers to power and charge an iPod, and view video and music libraries on individual seat-back displays. Originally KLM and Air France were reported to be part of the deal with Apple, but they later released statements explaining that they were only contemplating the possibility of incorporating such systems.', 'question': \"Where can people using iPods on planes view the device's interface?\"}, {'example_id': '56d43da72ccc5a1400d830c1', 'decoded_answer': 'individual seat - back displays', 'actual_answer': 'Sasha Fierce', 'context': 'Following the disbandment of Destiny\\'s Child in June 2005, she released her second solo album, B\\'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.', 'question': \"What is the name of Beyoncé's alter-ego?\"}, {'example_id': '56cd687562d2951400fa6592', 'decoded_answer': 'Sasha Fierce', 'actual_answer': 'January 8, 2004', 'context': 'On January 8, 2004, Hewlett-Packard (HP) announced that they would sell HP-branded iPods under a license agreement from Apple. Several new retail channels were used—including Wal-Mart—and these iPods eventually made up 5% of all iPod sales. In July 2005, HP stopped selling iPods due to unfavorable terms and conditions imposed by Apple.', 'question': 'When did HP unveil their own edition of the iPod?'}, {'example_id': '56be9add3aeaaa14008c9156', 'decoded_answer': 'January 8, 2004', 'actual_answer': \"New York's Roseland Ballroom\", 'context': 'Her fourth studio album 4 was released on June 28, 2011 in the US. 4 sold 310,000 copies in its first week and debuted atop the Billboard 200 chart, giving Beyoncé her fourth consecutive number-one album in the US. The album was preceded by two of its singles \"Run the World (Girls)\" and \"Best Thing I Never Had\", which both attained moderate success. The fourth single \"Love on Top\" was a commercial success in the US. 4 also produced four other singles; \"Party\", \"Countdown\", \"I Care\" and \"End of Time\". \"Eat, Play, Love\", a cover story written by Beyoncé for Essence that detailed her 2010 career break, won her a writing award from the New York Association of Black Journalists. In late 2011, she took the stage at New York\\'s Roseland Ballroom for four nights of special performances: the 4 Intimate Nights with Beyoncé concerts saw the performance of her 4 album to a standing room only.', 'question': 'in 2011, Beyonce performed for four nights where?'}, {'example_id': '56cf582eaab44d1400b8909c', 'decoded_answer': \"New York's Roseland Ballroom\", 'actual_answer': 'Polish', 'context': \"All of Chopin's compositions include the piano. Most are for solo piano, though he also wrote two piano concertos, a few chamber pieces, and some songs to Polish lyrics. His keyboard style is highly individual and often technically demanding; his own performances were noted for their nuance and sensitivity. Chopin invented the concept of instrumental ballade. His major piano works also include mazurkas, waltzes, nocturnes, polonaises, études, impromptus, scherzos, preludes and sonatas, some published only after his death. Influences on his compositional style include Polish folk music, the classical tradition of J. S. Bach, Mozart and Schubert, the music of all of whom he admired, as well as the Paris salons where he was a frequent guest. His innovations in style, musical form, and harmony, and his association of music with nationalism, were influential throughout and after the late Romantic period.\", 'question': 'Chopin composed several songs to lyrics of what language?'}, {'example_id': '56d45fcb2ccc5a1400d830fb', 'decoded_answer': 'Polish', 'actual_answer': 'No, No, No', 'context': 'The group changed their name to Destiny\\'s Child in 1996, based upon a passage in the Book of Isaiah. In 1997, Destiny\\'s Child released their major label debut song \"Killing Time\" on the soundtrack to the 1997 film, Men in Black. The following year, the group released their self-titled debut album, scoring their first major hit \"No, No, No\". The album established the group as a viable act in the music industry, with moderate sales and winning the group three Soul Train Lady of Soul Awards for Best R&B/Soul Album of the Year, Best R&B/Soul or Rap New Artist, and Best R&B/Soul Single for \"No, No, No\". The group released their multi-platinum second album The Writing\\'s on the Wall in 1999. The record features some of the group\\'s most widely known songs such as \"Bills, Bills, Bills\", the group\\'s first number-one single, \"Jumpin\\' Jumpin\\'\" and \"Say My Name\", which became their most successful song at the time, and would remain one of their signature songs. \"Say My Name\" won the Best R&B Performance by a Duo or Group with Vocals and the Best R&B Song at the 43rd Annual Grammy Awards. The Writing\\'s on the Wall sold more than eight million copies worldwide. During this time, Beyoncé recorded a duet with Marc Nelson, an original member of Boyz II Men, on the song \"After All Is Said and Done\" for the soundtrack to the 1999 film, The Best Man.', 'question': \"What was Destiny's Child's first major song hit?\"}, {'example_id': '56bf725c3aeaaa14008c9644', 'decoded_answer': 'No, No, No', 'actual_answer': 'Forbes', 'context': 'A self-described \"modern-day feminist\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny\\'s Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award\\'s history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.', 'question': 'What magazine rated Beyonce as the most powerful female musician in 2015?'}, {'example_id': '56cf67c74df3c31400b0d72e', 'decoded_answer': 'Forbes', 'actual_answer': 'Konstancja Gładkowska', 'context': 'Four boarders at his parents\\' apartments became Chopin\\'s intimates: Tytus Woyciechowski, Jan Nepomucen Białobłocki, Jan Matuszyński and Julian Fontana; the latter two would become part of his Paris milieu. He was friendly with members of Warsaw\\'s young artistic and intellectual world, including Fontana, Józef Bohdan Zaleski and Stefan Witwicki. He was also attracted to the singing student Konstancja Gładkowska. In letters to Woyciechowski, he indicated which of his works, and even which of their passages, were influenced by his fascination with her; his letter of 15 May 1830 revealed that the slow movement (Larghetto) of his Piano Concerto No. 1 (in E minor) was secretly dedicated to her – \"It should be like dreaming in beautiful springtime – by moonlight.\" His final Conservatory report (July 1829) read: \"Chopin F., third-year student, exceptional talent, musical genius.\"', 'question': 'Which singer did Chopin become fascinated with?'}, {'example_id': '56d3a85259d6e414001468ac', 'decoded_answer': 'Konstancja Gładkowska', 'actual_answer': \"Chopin's\", 'context': \"Two of Chopin's long-standing pupils, Karol Mikuli (1821–1897) and Georges Mathias, were themselves piano teachers and passed on details of his playing to their own students, some of whom (such as Raoul Koczalski) were to make recordings of his music. Other pianists and composers influenced by Chopin's style include Louis Moreau Gottschalk, Édouard Wolff (1816–1880) and Pierre Zimmermann. Debussy dedicated his own 1915 piano Études to the memory of Chopin; he frequently played Chopin's music during his studies at the Paris Conservatoire, and undertook the editing of Chopin's piano music for the publisher Jacques Durand.\", 'question': 'What music did Debussy play a lot at the Paris Conservatoire?'}, {'example_id': '56d38c2b59d6e41400146705', 'decoded_answer': \"Chopin's\", 'actual_answer': 'Père Lachaise Cemetery', 'context': \"Mozart's Requiem was sung at the funeral; the soloists were the soprano Jeanne-Anais Castellan, the mezzo-soprano Pauline Viardot, the tenor Alexis Dupont, and the bass Luigi Lablache; Chopin's Preludes No. 4 in E minor and No. 6 in B minor were also played. The organist at the funeral was Louis Lefébure-Wély. The funeral procession to Père Lachaise Cemetery, which included Chopin's sister Ludwika, was led by the aged Prince Adam Czartoryski. The pallbearers included Delacroix, Franchomme, and Camille Pleyel. At the graveside, the Funeral March from Chopin's Piano Sonata No. 2 was played, in Reber's instrumentation.\", 'question': 'Which cemetery was Chopin buried in?'}, {'example_id': '56bfa761a10cfb1400551205', 'decoded_answer': 'Père Lachaise Cemetery', 'actual_answer': 'Coldplay', 'context': 'At the 57th Annual Grammy Awards in February 2015, Beyoncé was nominated for six awards, ultimately winning three: Best R&B Performance and Best R&B Song for \"Drunk in Love\", and Best Surround Sound Album for Beyoncé. She was nominated for Album of the Year but the award was won by Beck for his Morning Phase album. In August, the cover of the September issue of Vogue magazine was unveiled online, Beyoncé as the cover star, becoming the first African-American artist and third African-American woman in general to cover the September issue. She headlined the 2015 Made in America festival in early September and also the Global Citizen Festival later that month. Beyoncé made an uncredited featured appearance on the track \"Hymn for the Weekend\" by British rock band Coldplay, on their seventh studio album A Head Full of Dreams (2015), which saw release in December. On January 7, 2016, Pepsi announced Beyoncé would perform alongside Coldplay at Super Bowl 50 in February. Knowles has previously performed at four Super Bowl shows throughout her career, serving as the main headliner of the 47th Super Bowl halftime show in 2013.', 'question': 'With what British band did Beyonce perform on their album?'}]\n"
     ]
    }
   ],
   "source": [
    "def retrieve_and_compare_answers(dataset, examples, split_type='train'):\n",
    "    answer_mismatches = []\n",
    "    counter = 0\n",
    "    for i, example in enumerate(examples):\n",
    "        # Retrieve stored start and end positions\n",
    "        start_pos = dataset[i]['start_positions']\n",
    "        end_pos = dataset[i]['end_positions']\n",
    "        \n",
    "        # Fetch the context and calculate predicted answer text\n",
    "        context = example['context']\n",
    "        question = example['question'] \n",
    "        answer_text = tokenizer.decode(dataset[i]['input_ids'][start_pos:end_pos])\n",
    "        \n",
    "        # Normalize and compare with actual answer\n",
    "        actual_answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "        #normalized_actual_answer = unidecode(actual_answer.lower().replace(\" \", \"\"))\n",
    "        #normalized_predicted_answer = unidecode(answer_text.lower().replace(\" \", \"\"))\n",
    "        normalized_actual_answer = actual_answer.lower().replace(\" \", \"\")\n",
    "        normalized_predicted_answer = answer_text.lower().replace(\" \", \"\")\n",
    "        \n",
    "        if normalized_actual_answer != normalized_predicted_answer:\n",
    "            counter += 1\n",
    "            answer_mismatches.append({\n",
    "                'example_id': example['id'],\n",
    "                'decoded_answer': answer_text,\n",
    "                'actual_answer': actual_answer,\n",
    "                'context': context,\n",
    "                'question': question\n",
    "            })\n",
    "        #if counter > 300: break\n",
    "    \n",
    "    print(f\"Number of mismatches in {split_type}: {len(answer_mismatches)}\")\n",
    "    return answer_mismatches\n",
    "\n",
    "# Retrieve mismatches for training and validation datasets\n",
    "train_mismatches = retrieve_and_compare_answers(train_dataset, squad_raw['train'], 'train')\n",
    "\n",
    "# Optionally, review some mismatches\n",
    "print(train_mismatches[:12])  # Print first 3 mismatches from training\n",
    "#print(validation_mismatches[:3])  # Print first 3 mismatches from validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "fb13f381-0373-4790-87c5-e56c55cd4907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mismatches in train: 51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw Example ID</th>\n",
       "      <th>Raw Question</th>\n",
       "      <th>Decoded Question</th>\n",
       "      <th>Decoded Answer</th>\n",
       "      <th>Actual Answer</th>\n",
       "      <th>Raw Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56cc84ab6d243a140015efe3</td>\n",
       "      <td>Where can people using iPods on planes view th...</td>\n",
       "      <td>Where can people using iPods on planes view th...</td>\n",
       "      <td>1833</td>\n",
       "      <td>individual seat-back displays</td>\n",
       "      <td>Beginning in mid-2007, four major airlines, Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56d43da72ccc5a1400d830c1</td>\n",
       "      <td>What is the name of Beyoncé's alter-ego?</td>\n",
       "      <td>What is the name of Beyoncé's alter-ego?</td>\n",
       "      <td>individual seat - back displays</td>\n",
       "      <td>Sasha Fierce</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56cd687562d2951400fa6592</td>\n",
       "      <td>When did HP unveil their own edition of the iPod?</td>\n",
       "      <td>When did HP unveil their own edition of the iPod?</td>\n",
       "      <td>Sasha Fierce</td>\n",
       "      <td>January 8, 2004</td>\n",
       "      <td>On January 8, 2004, Hewlett-Packard (HP) annou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56be9add3aeaaa14008c9156</td>\n",
       "      <td>in 2011, Beyonce performed for four nights where?</td>\n",
       "      <td>in 2011, Beyonce performed for four nights where?</td>\n",
       "      <td>January 8, 2004</td>\n",
       "      <td>New York's Roseland Ballroom</td>\n",
       "      <td>Her fourth studio album 4 was released on June...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56cf582eaab44d1400b8909c</td>\n",
       "      <td>Chopin composed several songs to lyrics of wha...</td>\n",
       "      <td>Chopin composed several songs to lyrics of wha...</td>\n",
       "      <td>New York's Roseland Ballroom</td>\n",
       "      <td>Polish</td>\n",
       "      <td>All of Chopin's compositions include the piano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56d45fcb2ccc5a1400d830fb</td>\n",
       "      <td>What was Destiny's Child's first major song hit?</td>\n",
       "      <td>What was Destiny's Child's first major song hit?</td>\n",
       "      <td>Polish</td>\n",
       "      <td>No, No, No</td>\n",
       "      <td>The group changed their name to Destiny's Chil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56bf725c3aeaaa14008c9644</td>\n",
       "      <td>What magazine rated Beyonce as the most powerf...</td>\n",
       "      <td>What magazine rated Beyonce as the most powerf...</td>\n",
       "      <td>No, No, No</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>A self-described \"modern-day feminist\", Beyonc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56cf67c74df3c31400b0d72e</td>\n",
       "      <td>Which singer did Chopin become fascinated with?</td>\n",
       "      <td>Which singer did Chopin become fascinated with?</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>Konstancja Gładkowska</td>\n",
       "      <td>Four boarders at his parents' apartments becam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56d3a85259d6e414001468ac</td>\n",
       "      <td>What music did Debussy play a lot at the Paris...</td>\n",
       "      <td>What music did Debussy play a lot at the Paris...</td>\n",
       "      <td>Konstancja Gładkowska</td>\n",
       "      <td>Chopin's</td>\n",
       "      <td>Two of Chopin's long-standing pupils, Karol Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>56d38c2b59d6e41400146705</td>\n",
       "      <td>Which cemetery was Chopin buried in?</td>\n",
       "      <td>Which cemetery was Chopin buried in?</td>\n",
       "      <td>Chopin's</td>\n",
       "      <td>Père Lachaise Cemetery</td>\n",
       "      <td>Mozart's Requiem was sung at the funeral; the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>56bfa761a10cfb1400551205</td>\n",
       "      <td>With what British band did Beyonce perform on ...</td>\n",
       "      <td>With what British band did Beyonce perform on ...</td>\n",
       "      <td>Père Lachaise Cemetery</td>\n",
       "      <td>Coldplay</td>\n",
       "      <td>At the 57th Annual Grammy Awards in February 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>56d1ca30e7d4791d009021a7</td>\n",
       "      <td>In what year was Chopin born?</td>\n",
       "      <td>In what year was Chopin born?</td>\n",
       "      <td>Coldplay</td>\n",
       "      <td>1810</td>\n",
       "      <td>Frédéric François Chopin (/ˈʃoʊpæn/; French pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>56ce88c8aab44d1400b8884b</td>\n",
       "      <td>What interface was gradually phased out for bo...</td>\n",
       "      <td>What interface was gradually phased out for bo...</td>\n",
       "      <td>1810</td>\n",
       "      <td>FireWire</td>\n",
       "      <td>The third generation began including a 30-pin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>56beabab3aeaaa14008c91dc</td>\n",
       "      <td>Beyonce supported which campaign that encourag...</td>\n",
       "      <td>Beyonce supported which campaign that encourag...</td>\n",
       "      <td>FireWire</td>\n",
       "      <td>Ban Bossy campaign</td>\n",
       "      <td>In an interview published by Vogue in April 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>56cc91b56d243a140015f030</td>\n",
       "      <td>What was the name of the event at which the iT...</td>\n",
       "      <td>What was the name of the event at which the iT...</td>\n",
       "      <td>Ban Bossy campaign</td>\n",
       "      <td>The Beat Goes On...</td>\n",
       "      <td>Apple debuted the iTunes Wi-Fi Music Store on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>56cbeb396d243a140015edeb</td>\n",
       "      <td>Who was Frédéric a guest of during his stay in...</td>\n",
       "      <td>Who was Frédéric a guest of during his stay in...</td>\n",
       "      <td>The Beat Goes On...</td>\n",
       "      <td>Prince Antoni Radziwiłł</td>\n",
       "      <td>In September 1828 Chopin, while still a studen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>56d22055e7d4791d00902685</td>\n",
       "      <td>What is th ename of the mechanical organ Chopi...</td>\n",
       "      <td>What is th ename of the mechanical organ Chopi...</td>\n",
       "      <td>Prince Antoni Radziwiłł</td>\n",
       "      <td>eolomelodicon</td>\n",
       "      <td>From September 1823 to 1826 Chopin attended th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>56cc06496d243a140015ee5a</td>\n",
       "      <td>Who was the host of the gathering where Frédér...</td>\n",
       "      <td>Who was the host of the gathering where Frédér...</td>\n",
       "      <td>eolomelodicon</td>\n",
       "      <td>Marie d'Agoult</td>\n",
       "      <td>In 1836, at a party hosted by Marie d'Agoult, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>56cf61d3aab44d1400b891a3</td>\n",
       "      <td>The Saxon Palace was taken over for military u...</td>\n",
       "      <td>The Saxon Palace was taken over for military u...</td>\n",
       "      <td>Marie d'Agoult</td>\n",
       "      <td>1817</td>\n",
       "      <td>In 1817 the Saxon Palace was requisitioned by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>56bed32f3aeaaa14008c94d3</td>\n",
       "      <td>Which two countries can you purchase Beyonce's...</td>\n",
       "      <td>Which two countries can you purchase Beyonce's...</td>\n",
       "      <td>1817</td>\n",
       "      <td>US and Canada</td>\n",
       "      <td>Beyoncé and her mother introduced House of Der...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>56bf9c70a10cfb14005511bb</td>\n",
       "      <td>In what year did Beyonce have her hiatus?</td>\n",
       "      <td>In what year did Beyonce have her hiatus?</td>\n",
       "      <td>US and Canada</td>\n",
       "      <td>2010</td>\n",
       "      <td>Beyoncé announced a hiatus from her music care...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>56d4e91b2ccc5a1400d83334</td>\n",
       "      <td>What year was Beyoncé featured both on the Tim...</td>\n",
       "      <td>What year was Beyoncé featured both on the Tim...</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>In The New Yorker music critic Jody Rosen desc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>56cbe5df6d243a140015edd6</td>\n",
       "      <td>What year did Frédéric leave Warsaw after movi...</td>\n",
       "      <td>What year did Frédéric leave Warsaw after movi...</td>\n",
       "      <td>2014</td>\n",
       "      <td>1830</td>\n",
       "      <td>In 1827, soon after the death of Chopin's youn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>56d3a3df59d6e41400146860</td>\n",
       "      <td>Who wrote a glowing review of Chopin's love fo...</td>\n",
       "      <td>Who wrote a glowing review of Chopin's love fo...</td>\n",
       "      <td>1830</td>\n",
       "      <td>Schumann</td>\n",
       "      <td>With his mazurkas and polonaises, Chopin has b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>56d1cf80e7d4791d00902212</td>\n",
       "      <td>What language were some songs written in that ...</td>\n",
       "      <td>What language were some songs written in that ...</td>\n",
       "      <td>Schumann</td>\n",
       "      <td>Polish</td>\n",
       "      <td>All of Chopin's compositions include the piano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>56d4f6922ccc5a1400d83397</td>\n",
       "      <td>What is the name of the House of Deréon junior...</td>\n",
       "      <td>What is the name of the House of Deréon junior...</td>\n",
       "      <td>Polish</td>\n",
       "      <td>Deréon.</td>\n",
       "      <td>Beyoncé and her mother introduced House of Der...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>56d43f7e2ccc5a1400d830c8</td>\n",
       "      <td>After leaving Destiny's Child, how many record...</td>\n",
       "      <td>After leaving Destiny's Child, how many record...</td>\n",
       "      <td>Deréon.</td>\n",
       "      <td>118 million</td>\n",
       "      <td>A self-described \"modern-day feminist\", Beyonc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>56cd6a3d62d2951400fa659d</td>\n",
       "      <td>Where did the Yongle Emperor send Yang Sanbao?</td>\n",
       "      <td>Where did the Yongle Emperor send Yang Sanbao?</td>\n",
       "      <td>118 million</td>\n",
       "      <td>Tibet</td>\n",
       "      <td>Shih-Shan Henry Tsai writes that the Yongle Em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>56bfbda3a10cfb140055129b</td>\n",
       "      <td>Where did she perform wearing Baker's hula skirt?</td>\n",
       "      <td>Where did she perform wearing Baker's hula skirt?</td>\n",
       "      <td>Tibet</td>\n",
       "      <td>2006 Fashion Rocks concert</td>\n",
       "      <td>The feminism and female empowerment themes on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>56d43f7e2ccc5a1400d830cb</td>\n",
       "      <td>What magazine named Beyoncé as the most powerf...</td>\n",
       "      <td>What magazine named Beyoncé as the most powerf...</td>\n",
       "      <td>2006 Fashion Rocks concert</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>A self-described \"modern-day feminist\", Beyonc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>56d22055e7d4791d00902686</td>\n",
       "      <td>What did Tsar Alexander I give to Chopin?</td>\n",
       "      <td>What did Tsar Alexander I give to Chopin?</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>a diamond ring.</td>\n",
       "      <td>From September 1823 to 1826 Chopin attended th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>56bf6e823aeaaa14008c962b</td>\n",
       "      <td>After what movie portraying Etta James, did Be...</td>\n",
       "      <td>After what movie portraying Etta James, did Be...</td>\n",
       "      <td>a diamond ring.</td>\n",
       "      <td>Cadillac Records</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>56d354c659d6e414001462d0</td>\n",
       "      <td>When did Chopin end his relationship with Sand?</td>\n",
       "      <td>When did Chopin end his relationship with Sand?</td>\n",
       "      <td>Cadillac Records</td>\n",
       "      <td>1847</td>\n",
       "      <td>Chopin's relations with Sand were soured in 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>1847</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>56cbe2fd6d243a140015edcc</td>\n",
       "      <td>In which village did Frédéric first experience...</td>\n",
       "      <td>In which village did Frédéric first experience...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>Szafarnia</td>\n",
       "      <td>During 1824–28 Chopin spent his vacations away...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>56d4bd272ccc5a1400d831a1</td>\n",
       "      <td>Beyoncé's role in Dreamgirls was based on what...</td>\n",
       "      <td>Beyoncé's role in Dreamgirls was based on what...</td>\n",
       "      <td>Szafarnia</td>\n",
       "      <td>Diana Ross.</td>\n",
       "      <td>Her first acting role of 2006 was in the comed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>56bfed855a85de14001c7868</td>\n",
       "      <td>What crowdfunding platform was used in the con...</td>\n",
       "      <td>What crowdfunding platform was used in the con...</td>\n",
       "      <td>Diana Ross.</td>\n",
       "      <td>Catapult</td>\n",
       "      <td>In December, Beyoncé along with a variety of o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>56ce1138aab44d1400b88428</td>\n",
       "      <td>Where did Chopin create the majority of his co...</td>\n",
       "      <td>Where did Chopin create the majority of his co...</td>\n",
       "      <td>Catapult</td>\n",
       "      <td>France</td>\n",
       "      <td>In his native Poland, in France, where he comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>56ce4a58aab44d1400b8866c</td>\n",
       "      <td>Who refused an audience with the Ü-Tsang king?</td>\n",
       "      <td>Who refused an audience with the Ü-Tsang king?</td>\n",
       "      <td>France</td>\n",
       "      <td>The fourth Dalai Lama</td>\n",
       "      <td>In 1565, the powerful Rinbung princes were ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>56bf79c73aeaaa14008c966e</td>\n",
       "      <td>In what year did Beyonce's father quit his job...</td>\n",
       "      <td>In what year did Beyonce's father quit his job...</td>\n",
       "      <td>The fourth Dalai Lama</td>\n",
       "      <td>1995</td>\n",
       "      <td>At age eight, Beyoncé and childhood friend Kel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>56d372e959d6e41400146403</td>\n",
       "      <td>What two dignitaries where at his first perfor...</td>\n",
       "      <td>What two dignitaries where at his first perfor...</td>\n",
       "      <td></td>\n",
       "      <td>Queen Victoria and Prince Albert.</td>\n",
       "      <td>In London Chopin took lodgings at Dover Street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>56d12d1e17492d1400aabb64</td>\n",
       "      <td>With the help of strong iPod sales, how much p...</td>\n",
       "      <td>With the help of strong iPod sales, how much p...</td>\n",
       "      <td>1995</td>\n",
       "      <td>$3.5 billion</td>\n",
       "      <td>On October 22, 2007, Apple reported quarterly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>56ce3406aab44d1400b88570</td>\n",
       "      <td>Who broke the eunuch influence at court?</td>\n",
       "      <td>Who broke the eunuch influence at court?</td>\n",
       "      <td>Queen Victoria and Prince Albert.</td>\n",
       "      <td>Yang Tinghe</td>\n",
       "      <td>During the reign of the Jiajing Emperor (r. 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>56cc52186d243a140015ef04</td>\n",
       "      <td>Where did Khubilai seek support as Emperor?</td>\n",
       "      <td>Where did Khubilai seek support as Emperor?</td>\n",
       "      <td>$ 3. 5 billion</td>\n",
       "      <td>China</td>\n",
       "      <td>Kublai Khan did not conquer the Song dynasty i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>56cd8d2962d2951400fa66ec</td>\n",
       "      <td>Who convinced the Ming to reopen their border ...</td>\n",
       "      <td>Who convinced the Ming to reopen their border ...</td>\n",
       "      <td>Yang Tinghe</td>\n",
       "      <td>Altan Khan</td>\n",
       "      <td>While the Ming dynasty traded horses with Tibe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>56d4d18d2ccc5a1400d8325a</td>\n",
       "      <td>How many people watched the 2011 MTV Video Mus...</td>\n",
       "      <td>How many people watched the 2011 MTV Video Mus...</td>\n",
       "      <td>China</td>\n",
       "      <td>12.4 million</td>\n",
       "      <td>In August, the couple attended the 2011 MTV Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>56d31f8a59d6e41400146266</td>\n",
       "      <td>What was the first name of the girl Chopin pro...</td>\n",
       "      <td>What was the first name of the girl Chopin pro...</td>\n",
       "      <td>Altan Khan</td>\n",
       "      <td>Maria</td>\n",
       "      <td>In the spring of 1834, Chopin attended the Low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>56beb67d3aeaaa14008c929b</td>\n",
       "      <td>An example of a song aimed towards a male audi...</td>\n",
       "      <td>An example of a song aimed towards a male audi...</td>\n",
       "      <td>12. 4 million</td>\n",
       "      <td>Cater 2 U</td>\n",
       "      <td>She has received co-writing credits for most o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>56ce3ed1aab44d1400b885fe</td>\n",
       "      <td>What practice did Altan Khan put to an end?</td>\n",
       "      <td>What practice did Altan Khan put to an end?</td>\n",
       "      <td>Maria</td>\n",
       "      <td>the native Mongol practices of shamanism and b...</td>\n",
       "      <td>Laird writes that Altan Khan abolished the nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>56cc06496d243a140015ee5e</td>\n",
       "      <td>Who did George Sand write to when admitting ha...</td>\n",
       "      <td>Who did George Sand write to when admitting ha...</td>\n",
       "      <td>Cater 2 U</td>\n",
       "      <td>Grzymała</td>\n",
       "      <td>In 1836, at a party hosted by Marie d'Agoult, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Raw Example ID  \\\n",
       "1   56cc84ab6d243a140015efe3   \n",
       "2   56d43da72ccc5a1400d830c1   \n",
       "3   56cd687562d2951400fa6592   \n",
       "4   56be9add3aeaaa14008c9156   \n",
       "5   56cf582eaab44d1400b8909c   \n",
       "6   56d45fcb2ccc5a1400d830fb   \n",
       "7   56bf725c3aeaaa14008c9644   \n",
       "8   56cf67c74df3c31400b0d72e   \n",
       "9   56d3a85259d6e414001468ac   \n",
       "10  56d38c2b59d6e41400146705   \n",
       "11  56bfa761a10cfb1400551205   \n",
       "12  56d1ca30e7d4791d009021a7   \n",
       "13  56ce88c8aab44d1400b8884b   \n",
       "14  56beabab3aeaaa14008c91dc   \n",
       "15  56cc91b56d243a140015f030   \n",
       "16  56cbeb396d243a140015edeb   \n",
       "17  56d22055e7d4791d00902685   \n",
       "18  56cc06496d243a140015ee5a   \n",
       "19  56cf61d3aab44d1400b891a3   \n",
       "20  56bed32f3aeaaa14008c94d3   \n",
       "21  56bf9c70a10cfb14005511bb   \n",
       "22  56d4e91b2ccc5a1400d83334   \n",
       "23  56cbe5df6d243a140015edd6   \n",
       "24  56d3a3df59d6e41400146860   \n",
       "25  56d1cf80e7d4791d00902212   \n",
       "26  56d4f6922ccc5a1400d83397   \n",
       "27  56d43f7e2ccc5a1400d830c8   \n",
       "28  56cd6a3d62d2951400fa659d   \n",
       "29  56bfbda3a10cfb140055129b   \n",
       "30  56d43f7e2ccc5a1400d830cb   \n",
       "31  56d22055e7d4791d00902686   \n",
       "32  56bf6e823aeaaa14008c962b   \n",
       "33  56d354c659d6e414001462d0   \n",
       "34  56be85543aeaaa14008c9065   \n",
       "35  56cbe2fd6d243a140015edcc   \n",
       "36  56d4bd272ccc5a1400d831a1   \n",
       "37  56bfed855a85de14001c7868   \n",
       "38  56ce1138aab44d1400b88428   \n",
       "39  56ce4a58aab44d1400b8866c   \n",
       "40  56bf79c73aeaaa14008c966e   \n",
       "41  56d372e959d6e41400146403   \n",
       "42  56d12d1e17492d1400aabb64   \n",
       "43  56ce3406aab44d1400b88570   \n",
       "44  56cc52186d243a140015ef04   \n",
       "45  56cd8d2962d2951400fa66ec   \n",
       "46  56d4d18d2ccc5a1400d8325a   \n",
       "47  56d31f8a59d6e41400146266   \n",
       "48  56beb67d3aeaaa14008c929b   \n",
       "49  56ce3ed1aab44d1400b885fe   \n",
       "50  56cc06496d243a140015ee5e   \n",
       "\n",
       "                                         Raw Question  \\\n",
       "1   Where can people using iPods on planes view th...   \n",
       "2            What is the name of Beyoncé's alter-ego?   \n",
       "3   When did HP unveil their own edition of the iPod?   \n",
       "4   in 2011, Beyonce performed for four nights where?   \n",
       "5   Chopin composed several songs to lyrics of wha...   \n",
       "6    What was Destiny's Child's first major song hit?   \n",
       "7   What magazine rated Beyonce as the most powerf...   \n",
       "8     Which singer did Chopin become fascinated with?   \n",
       "9   What music did Debussy play a lot at the Paris...   \n",
       "10               Which cemetery was Chopin buried in?   \n",
       "11  With what British band did Beyonce perform on ...   \n",
       "12                      In what year was Chopin born?   \n",
       "13  What interface was gradually phased out for bo...   \n",
       "14  Beyonce supported which campaign that encourag...   \n",
       "15  What was the name of the event at which the iT...   \n",
       "16  Who was Frédéric a guest of during his stay in...   \n",
       "17  What is th ename of the mechanical organ Chopi...   \n",
       "18  Who was the host of the gathering where Frédér...   \n",
       "19  The Saxon Palace was taken over for military u...   \n",
       "20  Which two countries can you purchase Beyonce's...   \n",
       "21          In what year did Beyonce have her hiatus?   \n",
       "22  What year was Beyoncé featured both on the Tim...   \n",
       "23  What year did Frédéric leave Warsaw after movi...   \n",
       "24  Who wrote a glowing review of Chopin's love fo...   \n",
       "25  What language were some songs written in that ...   \n",
       "26  What is the name of the House of Deréon junior...   \n",
       "27  After leaving Destiny's Child, how many record...   \n",
       "28     Where did the Yongle Emperor send Yang Sanbao?   \n",
       "29  Where did she perform wearing Baker's hula skirt?   \n",
       "30  What magazine named Beyoncé as the most powerf...   \n",
       "31          What did Tsar Alexander I give to Chopin?   \n",
       "32  After what movie portraying Etta James, did Be...   \n",
       "33    When did Chopin end his relationship with Sand?   \n",
       "34  What areas did Beyonce compete in when she was...   \n",
       "35  In which village did Frédéric first experience...   \n",
       "36  Beyoncé's role in Dreamgirls was based on what...   \n",
       "37  What crowdfunding platform was used in the con...   \n",
       "38  Where did Chopin create the majority of his co...   \n",
       "39     Who refused an audience with the Ü-Tsang king?   \n",
       "40  In what year did Beyonce's father quit his job...   \n",
       "41  What two dignitaries where at his first perfor...   \n",
       "42  With the help of strong iPod sales, how much p...   \n",
       "43           Who broke the eunuch influence at court?   \n",
       "44        Where did Khubilai seek support as Emperor?   \n",
       "45  Who convinced the Ming to reopen their border ...   \n",
       "46  How many people watched the 2011 MTV Video Mus...   \n",
       "47  What was the first name of the girl Chopin pro...   \n",
       "48  An example of a song aimed towards a male audi...   \n",
       "49        What practice did Altan Khan put to an end?   \n",
       "50  Who did George Sand write to when admitting ha...   \n",
       "\n",
       "                                     Decoded Question  \\\n",
       "1   Where can people using iPods on planes view th...   \n",
       "2            What is the name of Beyoncé's alter-ego?   \n",
       "3   When did HP unveil their own edition of the iPod?   \n",
       "4   in 2011, Beyonce performed for four nights where?   \n",
       "5   Chopin composed several songs to lyrics of wha...   \n",
       "6    What was Destiny's Child's first major song hit?   \n",
       "7   What magazine rated Beyonce as the most powerf...   \n",
       "8     Which singer did Chopin become fascinated with?   \n",
       "9   What music did Debussy play a lot at the Paris...   \n",
       "10               Which cemetery was Chopin buried in?   \n",
       "11  With what British band did Beyonce perform on ...   \n",
       "12                      In what year was Chopin born?   \n",
       "13  What interface was gradually phased out for bo...   \n",
       "14  Beyonce supported which campaign that encourag...   \n",
       "15  What was the name of the event at which the iT...   \n",
       "16  Who was Frédéric a guest of during his stay in...   \n",
       "17  What is th ename of the mechanical organ Chopi...   \n",
       "18  Who was the host of the gathering where Frédér...   \n",
       "19  The Saxon Palace was taken over for military u...   \n",
       "20  Which two countries can you purchase Beyonce's...   \n",
       "21          In what year did Beyonce have her hiatus?   \n",
       "22  What year was Beyoncé featured both on the Tim...   \n",
       "23  What year did Frédéric leave Warsaw after movi...   \n",
       "24  Who wrote a glowing review of Chopin's love fo...   \n",
       "25  What language were some songs written in that ...   \n",
       "26  What is the name of the House of Deréon junior...   \n",
       "27  After leaving Destiny's Child, how many record...   \n",
       "28     Where did the Yongle Emperor send Yang Sanbao?   \n",
       "29  Where did she perform wearing Baker's hula skirt?   \n",
       "30  What magazine named Beyoncé as the most powerf...   \n",
       "31          What did Tsar Alexander I give to Chopin?   \n",
       "32  After what movie portraying Etta James, did Be...   \n",
       "33    When did Chopin end his relationship with Sand?   \n",
       "34  What areas did Beyonce compete in when she was...   \n",
       "35  In which village did Frédéric first experience...   \n",
       "36  Beyoncé's role in Dreamgirls was based on what...   \n",
       "37  What crowdfunding platform was used in the con...   \n",
       "38  Where did Chopin create the majority of his co...   \n",
       "39     Who refused an audience with the Ü-Tsang king?   \n",
       "40  In what year did Beyonce's father quit his job...   \n",
       "41  What two dignitaries where at his first perfor...   \n",
       "42  With the help of strong iPod sales, how much p...   \n",
       "43           Who broke the eunuch influence at court?   \n",
       "44        Where did Khubilai seek support as Emperor?   \n",
       "45  Who convinced the Ming to reopen their border ...   \n",
       "46  How many people watched the 2011 MTV Video Mus...   \n",
       "47  What was the first name of the girl Chopin pro...   \n",
       "48  An example of a song aimed towards a male audi...   \n",
       "49        What practice did Altan Khan put to an end?   \n",
       "50  Who did George Sand write to when admitting ha...   \n",
       "\n",
       "                       Decoded Answer  \\\n",
       "1                                1833   \n",
       "2     individual seat - back displays   \n",
       "3                        Sasha Fierce   \n",
       "4                     January 8, 2004   \n",
       "5        New York's Roseland Ballroom   \n",
       "6                              Polish   \n",
       "7                          No, No, No   \n",
       "8                              Forbes   \n",
       "9               Konstancja Gładkowska   \n",
       "10                           Chopin's   \n",
       "11             Père Lachaise Cemetery   \n",
       "12                           Coldplay   \n",
       "13                               1810   \n",
       "14                           FireWire   \n",
       "15                 Ban Bossy campaign   \n",
       "16                The Beat Goes On...   \n",
       "17            Prince Antoni Radziwiłł   \n",
       "18                      eolomelodicon   \n",
       "19                     Marie d'Agoult   \n",
       "20                               1817   \n",
       "21                      US and Canada   \n",
       "22                               2010   \n",
       "23                               2014   \n",
       "24                               1830   \n",
       "25                           Schumann   \n",
       "26                             Polish   \n",
       "27                            Deréon.   \n",
       "28                        118 million   \n",
       "29                              Tibet   \n",
       "30         2006 Fashion Rocks concert   \n",
       "31                             Forbes   \n",
       "32                    a diamond ring.   \n",
       "33                   Cadillac Records   \n",
       "34                               1847   \n",
       "35                singing and dancing   \n",
       "36                          Szafarnia   \n",
       "37                        Diana Ross.   \n",
       "38                           Catapult   \n",
       "39                             France   \n",
       "40              The fourth Dalai Lama   \n",
       "41                                      \n",
       "42                               1995   \n",
       "43  Queen Victoria and Prince Albert.   \n",
       "44                     $ 3. 5 billion   \n",
       "45                        Yang Tinghe   \n",
       "46                              China   \n",
       "47                         Altan Khan   \n",
       "48                      12. 4 million   \n",
       "49                              Maria   \n",
       "50                          Cater 2 U   \n",
       "\n",
       "                                        Actual Answer  \\\n",
       "1                       individual seat-back displays   \n",
       "2                                        Sasha Fierce   \n",
       "3                                     January 8, 2004   \n",
       "4                        New York's Roseland Ballroom   \n",
       "5                                              Polish   \n",
       "6                                          No, No, No   \n",
       "7                                              Forbes   \n",
       "8                               Konstancja Gładkowska   \n",
       "9                                            Chopin's   \n",
       "10                             Père Lachaise Cemetery   \n",
       "11                                           Coldplay   \n",
       "12                                               1810   \n",
       "13                                           FireWire   \n",
       "14                                 Ban Bossy campaign   \n",
       "15                                The Beat Goes On...   \n",
       "16                            Prince Antoni Radziwiłł   \n",
       "17                                      eolomelodicon   \n",
       "18                                     Marie d'Agoult   \n",
       "19                                               1817   \n",
       "20                                      US and Canada   \n",
       "21                                               2010   \n",
       "22                                               2014   \n",
       "23                                               1830   \n",
       "24                                           Schumann   \n",
       "25                                             Polish   \n",
       "26                                            Deréon.   \n",
       "27                                        118 million   \n",
       "28                                              Tibet   \n",
       "29                         2006 Fashion Rocks concert   \n",
       "30                                             Forbes   \n",
       "31                                    a diamond ring.   \n",
       "32                                   Cadillac Records   \n",
       "33                                               1847   \n",
       "34                                singing and dancing   \n",
       "35                                          Szafarnia   \n",
       "36                                        Diana Ross.   \n",
       "37                                           Catapult   \n",
       "38                                             France   \n",
       "39                              The fourth Dalai Lama   \n",
       "40                                               1995   \n",
       "41                  Queen Victoria and Prince Albert.   \n",
       "42                                       $3.5 billion   \n",
       "43                                        Yang Tinghe   \n",
       "44                                              China   \n",
       "45                                         Altan Khan   \n",
       "46                                       12.4 million   \n",
       "47                                              Maria   \n",
       "48                                          Cater 2 U   \n",
       "49  the native Mongol practices of shamanism and b...   \n",
       "50                                           Grzymała   \n",
       "\n",
       "                                          Raw Context  \n",
       "1   Beginning in mid-2007, four major airlines, Un...  \n",
       "2   Following the disbandment of Destiny's Child i...  \n",
       "3   On January 8, 2004, Hewlett-Packard (HP) annou...  \n",
       "4   Her fourth studio album 4 was released on June...  \n",
       "5   All of Chopin's compositions include the piano...  \n",
       "6   The group changed their name to Destiny's Chil...  \n",
       "7   A self-described \"modern-day feminist\", Beyonc...  \n",
       "8   Four boarders at his parents' apartments becam...  \n",
       "9   Two of Chopin's long-standing pupils, Karol Mi...  \n",
       "10  Mozart's Requiem was sung at the funeral; the ...  \n",
       "11  At the 57th Annual Grammy Awards in February 2...  \n",
       "12  Frédéric François Chopin (/ˈʃoʊpæn/; French pr...  \n",
       "13  The third generation began including a 30-pin ...  \n",
       "14  In an interview published by Vogue in April 20...  \n",
       "15  Apple debuted the iTunes Wi-Fi Music Store on ...  \n",
       "16  In September 1828 Chopin, while still a studen...  \n",
       "17  From September 1823 to 1826 Chopin attended th...  \n",
       "18  In 1836, at a party hosted by Marie d'Agoult, ...  \n",
       "19  In 1817 the Saxon Palace was requisitioned by ...  \n",
       "20  Beyoncé and her mother introduced House of Der...  \n",
       "21  Beyoncé announced a hiatus from her music care...  \n",
       "22  In The New Yorker music critic Jody Rosen desc...  \n",
       "23  In 1827, soon after the death of Chopin's youn...  \n",
       "24  With his mazurkas and polonaises, Chopin has b...  \n",
       "25  All of Chopin's compositions include the piano...  \n",
       "26  Beyoncé and her mother introduced House of Der...  \n",
       "27  A self-described \"modern-day feminist\", Beyonc...  \n",
       "28  Shih-Shan Henry Tsai writes that the Yongle Em...  \n",
       "29  The feminism and female empowerment themes on ...  \n",
       "30  A self-described \"modern-day feminist\", Beyonc...  \n",
       "31  From September 1823 to 1826 Chopin attended th...  \n",
       "32  Following the disbandment of Destiny's Child i...  \n",
       "33  Chopin's relations with Sand were soured in 18...  \n",
       "34  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "35  During 1824–28 Chopin spent his vacations away...  \n",
       "36  Her first acting role of 2006 was in the comed...  \n",
       "37  In December, Beyoncé along with a variety of o...  \n",
       "38  In his native Poland, in France, where he comp...  \n",
       "39  In 1565, the powerful Rinbung princes were ove...  \n",
       "40  At age eight, Beyoncé and childhood friend Kel...  \n",
       "41  In London Chopin took lodgings at Dover Street...  \n",
       "42  On October 22, 2007, Apple reported quarterly ...  \n",
       "43  During the reign of the Jiajing Emperor (r. 15...  \n",
       "44  Kublai Khan did not conquer the Song dynasty i...  \n",
       "45  While the Ming dynasty traded horses with Tibe...  \n",
       "46  In August, the couple attended the 2011 MTV Vi...  \n",
       "47  In the spring of 1834, Chopin attended the Low...  \n",
       "48  She has received co-writing credits for most o...  \n",
       "49  Laird writes that Altan Khan abolished the nat...  \n",
       "50  In 1836, at a party hosted by Marie d'Agoult, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def retrieve_and_compare_answers(dataset, examples, split_type='train'):\n",
    "    answer_mismatches = []\n",
    "    counter = 0\n",
    "    for i, example in enumerate(examples):\n",
    "        # Retrieve stored start and end positions\n",
    "        start_pos = dataset[i]['start_positions']\n",
    "        end_pos = dataset[i]['end_positions']\n",
    "        \n",
    "        # Fetch the context and calculate predicted answer text\n",
    "        context = example['context']\n",
    "        question = example['question']\n",
    "        answer_text = tokenizer.decode(dataset[i]['input_ids'][start_pos:end_pos])\n",
    "        \n",
    "        # Normalize and compare with actual answer\n",
    "        actual_answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "        normalized_actual_answer = unidecode(actual_answer.lower().replace(\" \", \"\"))\n",
    "        normalized_predicted_answer = unidecode(answer_text.lower().replace(\" \", \"\"))\n",
    "        \n",
    "        if normalized_actual_answer != normalized_predicted_answer:\n",
    "            counter += 1\n",
    "            answer_mismatches.append({\n",
    "                'Raw Example ID': example['id'],\n",
    "                'Raw Question': question,\n",
    "                'Decoded Question': question,\n",
    "                'Decoded Answer': answer_text,\n",
    "                'Actual Answer': actual_answer,\n",
    "                'Raw Context': context[:200] + '...'  # Truncating context for display purposes\n",
    "            })\n",
    "        if counter > 50:\n",
    "            break\n",
    "    \n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    mismatches_df = pd.DataFrame(answer_mismatches)\n",
    "    print(f\"Number of mismatches in {split_type}: {len(answer_mismatches)}\")\n",
    "    return mismatches_df\n",
    "\n",
    "# Retrieve mismatches for training and validation datasets\n",
    "train_mismatches = retrieve_and_compare_answers(train_dataset, squad_raw['train'], 'train')\n",
    "# Uncomment and adjust as needed for validation dataset\n",
    "#validation_mismatches = retrieve_and_compare_answers(eval_dataset, squad_raw['validation'], 'validation')\n",
    "\n",
    "# Optionally, display some mismatches in a DataFrame\n",
    "display(train_mismatches.tail(50))  # Display first 12 mismatches from training\n",
    "#display(validation_mismatches.head(3))  # Display first 3 mismatches from validation if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05e3a780-9dc7-48fb-92f0-5c462b93df59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Who is M's rival?\"]\n",
      "['Following Garreth Mallory\\'s promotion to M, on a mission in Mexico City unofficially ordered by a posthumous message from the previous M, 007 James Bond kills three men plotting a terrorist bombing during the Day of the Dead and gives chase to Marco Sciarra, an assassin who survived the attack. In the ensuing struggle, Bond steals his ring, which is emblazoned with a stylised octopus, and then kills Sciarra by kicking him out of a helicopter. Upon returning to London, Bond is indefinitely suspended from field duty by M, who is in the midst of a power struggle with C, the head of the privately-backed Joint Intelligence Service, consisting of the recently merged MI5 and MI6. C campaigns for Britain to form alongside 8 other countries \"Nine Eyes \", a global surveillance and intelligence co-operation initiative between nine member states, and uses his influence to close down the \\'00\\' section, believing it to be outdated.']\n",
      "[{'text': ['C'], 'answer_start': [67]}]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(squad_raw['train'])\n",
    "id = '56cdcf7d62d2951400fa686e'\n",
    "print(df[df.id==id].question.values)\n",
    "print(df[df.id==id].context.values)\n",
    "print(df[df.id==id].answers.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "03599949-cdbe-47d0-8475-19b0a6d6a049",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mismatches in train: 51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Example ID</th>\n",
       "      <th>Total Tokens count</th>\n",
       "      <th>Question</th>\n",
       "      <th>Decode Answer</th>\n",
       "      <th>Actual Answer</th>\n",
       "      <th>start_pos</th>\n",
       "      <th>end_pos</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56be85543aeaaa14008c9063</td>\n",
       "      <td>173</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td>71</td>\n",
       "      <td>74</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56be85543aeaaa14008c9065</td>\n",
       "      <td>177</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56be85543aeaaa14008c9066</td>\n",
       "      <td>177</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td></td>\n",
       "      <td>2003</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
       "      <td>175</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston,</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>54</td>\n",
       "      <td>56</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
       "      <td>172</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9603</td>\n",
       "      <td>176</td>\n",
       "      <td>In what R&amp;B group was she the lead singer?</td>\n",
       "      <td>Destiny's</td>\n",
       "      <td>Destiny's Child</td>\n",
       "      <td>88</td>\n",
       "      <td>91</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9604</td>\n",
       "      <td>173</td>\n",
       "      <td>What album made her a worldwide known artist?</td>\n",
       "      <td>Dangerously in</td>\n",
       "      <td>Dangerously in Love</td>\n",
       "      <td>130</td>\n",
       "      <td>133</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56bf6b0f3aeaaa14008c9605</td>\n",
       "      <td>172</td>\n",
       "      <td>Who managed the Destiny's Child group?</td>\n",
       "      <td>Mathew</td>\n",
       "      <td>Mathew Knowles</td>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56d43c5f2ccc5a1400d830a9</td>\n",
       "      <td>169</td>\n",
       "      <td>When did Beyoncé rise to fame?</td>\n",
       "      <td>late</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56d43c5f2ccc5a1400d830aa</td>\n",
       "      <td>173</td>\n",
       "      <td>What role did Beyoncé have in Destiny's Child?</td>\n",
       "      <td>lead</td>\n",
       "      <td>lead singer</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>56d43c5f2ccc5a1400d830ab</td>\n",
       "      <td>176</td>\n",
       "      <td>What was the first album Beyoncé released as a...</td>\n",
       "      <td>Dangerously in</td>\n",
       "      <td>Dangerously in Love</td>\n",
       "      <td>133</td>\n",
       "      <td>136</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>56d43c5f2ccc5a1400d830ac</td>\n",
       "      <td>170</td>\n",
       "      <td>When did Beyoncé release Dangerously in Love?</td>\n",
       "      <td></td>\n",
       "      <td>2003</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>56d43c5f2ccc5a1400d830ad</td>\n",
       "      <td>174</td>\n",
       "      <td>How many Grammy awards did Beyoncé win for her...</td>\n",
       "      <td></td>\n",
       "      <td>five</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>56d43ce42ccc5a1400d830b4</td>\n",
       "      <td>174</td>\n",
       "      <td>What was Beyoncé's role in Destiny's Child?</td>\n",
       "      <td>lead</td>\n",
       "      <td>lead singer</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>56d43ce42ccc5a1400d830b5</td>\n",
       "      <td>176</td>\n",
       "      <td>What was the name of Beyoncé's first solo album?</td>\n",
       "      <td>Dangerously in</td>\n",
       "      <td>Dangerously in Love</td>\n",
       "      <td>133</td>\n",
       "      <td>136</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>56be86cf3aeaaa14008c9076</td>\n",
       "      <td>257</td>\n",
       "      <td>After her second solo album, what other entert...</td>\n",
       "      <td></td>\n",
       "      <td>acting</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>56be86cf3aeaaa14008c9078</td>\n",
       "      <td>250</td>\n",
       "      <td>Which artist did Beyonce marry?</td>\n",
       "      <td>Jay</td>\n",
       "      <td>Jay Z</td>\n",
       "      <td>107</td>\n",
       "      <td>108</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>56be86cf3aeaaa14008c9079</td>\n",
       "      <td>257</td>\n",
       "      <td>To set the record for Grammys, how many did Be...</td>\n",
       "      <td></td>\n",
       "      <td>six</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>56bf6e823aeaaa14008c9627</td>\n",
       "      <td>257</td>\n",
       "      <td>For what movie did Beyonce receive  her first ...</td>\n",
       "      <td>Dreamgirl</td>\n",
       "      <td>Dreamgirls</td>\n",
       "      <td>84</td>\n",
       "      <td>86</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>56bf6e823aeaaa14008c9629</td>\n",
       "      <td>259</td>\n",
       "      <td>When did Beyonce take a hiatus in her career a...</td>\n",
       "      <td></td>\n",
       "      <td>2010</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>Following the disbandment of Destiny's Child i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Example ID  Total Tokens count  \\\n",
       "0   56be85543aeaaa14008c9063                 173   \n",
       "1   56be85543aeaaa14008c9065                 177   \n",
       "2   56be85543aeaaa14008c9066                 177   \n",
       "3   56bf6b0f3aeaaa14008c9601                 175   \n",
       "4   56bf6b0f3aeaaa14008c9602                 172   \n",
       "5   56bf6b0f3aeaaa14008c9603                 176   \n",
       "6   56bf6b0f3aeaaa14008c9604                 173   \n",
       "7   56bf6b0f3aeaaa14008c9605                 172   \n",
       "8   56d43c5f2ccc5a1400d830a9                 169   \n",
       "9   56d43c5f2ccc5a1400d830aa                 173   \n",
       "10  56d43c5f2ccc5a1400d830ab                 176   \n",
       "11  56d43c5f2ccc5a1400d830ac                 170   \n",
       "12  56d43c5f2ccc5a1400d830ad                 174   \n",
       "13  56d43ce42ccc5a1400d830b4                 174   \n",
       "14  56d43ce42ccc5a1400d830b5                 176   \n",
       "15  56be86cf3aeaaa14008c9076                 257   \n",
       "16  56be86cf3aeaaa14008c9078                 250   \n",
       "17  56be86cf3aeaaa14008c9079                 257   \n",
       "18  56bf6e823aeaaa14008c9627                 257   \n",
       "19  56bf6e823aeaaa14008c9629                 259   \n",
       "\n",
       "                                             Question   Decode Answer  \\\n",
       "0            When did Beyonce start becoming popular?     in the late   \n",
       "1   What areas did Beyonce compete in when she was...     singing and   \n",
       "2   When did Beyonce leave Destiny's Child and bec...                   \n",
       "3       In what city and state did Beyonce  grow up?         Houston,   \n",
       "4          In which decade did Beyonce become famous?            late   \n",
       "5          In what R&B group was she the lead singer?       Destiny's   \n",
       "6       What album made her a worldwide known artist?  Dangerously in   \n",
       "7              Who managed the Destiny's Child group?          Mathew   \n",
       "8                      When did Beyoncé rise to fame?            late   \n",
       "9      What role did Beyoncé have in Destiny's Child?            lead   \n",
       "10  What was the first album Beyoncé released as a...  Dangerously in   \n",
       "11      When did Beyoncé release Dangerously in Love?                   \n",
       "12  How many Grammy awards did Beyoncé win for her...                   \n",
       "13        What was Beyoncé's role in Destiny's Child?            lead   \n",
       "14   What was the name of Beyoncé's first solo album?  Dangerously in   \n",
       "15  After her second solo album, what other entert...                   \n",
       "16                    Which artist did Beyonce marry?             Jay   \n",
       "17  To set the record for Grammys, how many did Be...                   \n",
       "18  For what movie did Beyonce receive  her first ...       Dreamgirl   \n",
       "19  When did Beyonce take a hiatus in her career a...                   \n",
       "\n",
       "          Actual Answer  start_pos  end_pos  \\\n",
       "0     in the late 1990s         71       74   \n",
       "1   singing and dancing         64       66   \n",
       "2                  2003        142      142   \n",
       "3        Houston, Texas         54       56   \n",
       "4            late 1990s         74       75   \n",
       "5       Destiny's Child         88       91   \n",
       "6   Dangerously in Love        130      133   \n",
       "7        Mathew Knowles         96       98   \n",
       "8            late 1990s         71       72   \n",
       "9           lead singer         78       79   \n",
       "10  Dangerously in Love        133      136   \n",
       "11                 2003        135      135   \n",
       "12                 five        152      152   \n",
       "13          lead singer         79       80   \n",
       "14  Dangerously in Love        133      136   \n",
       "15               acting         76       76   \n",
       "16                Jay Z        107      108   \n",
       "17                  six        166      166   \n",
       "18           Dreamgirls         84       86   \n",
       "19                 2010        172      172   \n",
       "\n",
       "                                              Context  \n",
       "0   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "1   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "2   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "3   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "4   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "5   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "6   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "7   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "8   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "9   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "10  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "11  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "12  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "13  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "14  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...  \n",
       "15  Following the disbandment of Destiny's Child i...  \n",
       "16  Following the disbandment of Destiny's Child i...  \n",
       "17  Following the disbandment of Destiny's Child i...  \n",
       "18  Following the disbandment of Destiny's Child i...  \n",
       "19  Following the disbandment of Destiny's Child i...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def retrieve_and_compare_answers(dataset, examples, split_type='train'):\n",
    "    answer_mismatches = []\n",
    "    counter = 0\n",
    "    for i, example in enumerate(examples):\n",
    "        # Retrieve stored start and end positions\n",
    "        start_pos = dataset[i]['start_positions']\n",
    "        end_pos = dataset[i]['end_positions']\n",
    "        \n",
    "        # Fetch the context and calculate predicted answer text\n",
    "        context = example['context']\n",
    "        question = example['question']\n",
    "        answer_text = tokenizer.decode(dataset[i]['input_ids'][start_pos:end_pos])\n",
    "        \n",
    "        # Normalize and compare with actual answer\n",
    "        actual_answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "        normalized_actual_answer = unidecode(actual_answer.lower().replace(\" \", \"\"))\n",
    "        normalized_predicted_answer = unidecode(answer_text.lower().replace(\" \", \"\"))\n",
    "        \n",
    "        # Tokenize to count the number of tokens\n",
    "        token_count_question = len(tokenizer.tokenize(question))\n",
    "        token_count_answer = len(tokenizer.tokenize(answer_text))\n",
    "        token_count_context = len(tokenizer.tokenize(context))\n",
    "        \n",
    "        if normalized_actual_answer != normalized_predicted_answer:\n",
    "            answer_mismatches.append({\n",
    "                'Example ID': example['id'],\n",
    "                'Total Tokens count' : token_count_question + token_count_answer + token_count_context,\n",
    "                'Question': question,\n",
    "                'Decode Answer': answer_text,\n",
    "                'Actual Answer': actual_answer,\n",
    "                'start_pos':start_pos,\n",
    "                'end_pos':end_pos,\n",
    "                'Context': context[:200] + '...'  # Truncating context for display purposes\n",
    "            })\n",
    "            counter += 1\n",
    "            if counter > 50: break\n",
    "    \n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    mismatches_df = pd.DataFrame(answer_mismatches)\n",
    "    print(f\"Number of mismatches in {split_type}: {len(answer_mismatches)}\")\n",
    "    return mismatches_df\n",
    "\n",
    "# Retrieve mismatches for training and validation datasets\n",
    "train_mismatches = retrieve_and_compare_answers(train_dataset, squad_raw['train'], 'train')\n",
    "# Uncomment and adjust as needed for validation dataset\n",
    "#validation_mismatches = retrieve_and_compare_answers(eval_dataset, squad_raw['validation'], 'validation')\n",
    "\n",
    "# Optionally, display some mismatches in a DataFrame\n",
    "display(train_mismatches.head(20))  # Display first 12 mismatches from training\n",
    "#display(validation_mismatches.head(3))  # Display first 3 mismatches from validation if needed\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f921c47b-c04b-4645-ac5a-e024234b1c6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "def collect_mismatch_ids(dataset, examples):\n",
    "    mismatch_ids = []\n",
    "    for i, example in enumerate(examples):        \n",
    "        # Retrieve stored start and end positions\n",
    "        start_pos = dataset[i]['start_positions']\n",
    "        end_pos = dataset[i]['end_positions']\n",
    "        \n",
    "        # Fetch the context and calculate predicted answer text\n",
    "        answer_text = tokenizer.decode(dataset[i]['input_ids'][start_pos:end_pos])\n",
    "        \n",
    "        # Normalize and compare with actual answer\n",
    "        actual_answer = example['answers']['text'][0] if example['answers']['text'] else \"\"\n",
    "        normalized_actual_answer = unidecode(actual_answer.lower().replace(\" \", \"\"))\n",
    "        normalized_predicted_answer = unidecode(answer_text.lower().replace(\" \", \"\"))\n",
    "        \n",
    "        if normalized_actual_answer != normalized_predicted_answer:\n",
    "            mismatch_ids.append(example['id'])\n",
    "        \n",
    "            \n",
    "    return mismatch_ids\n",
    "\n",
    "def remove_mismatches(dataset, mismatch_ids):\n",
    "    # Filter the dataset to exclude mismatched entries\n",
    "    filtered_dataset = dataset.filter(lambda example: example['id'] not in mismatch_ids)\n",
    "    return filtered_dataset\n",
    "\n",
    "# Collect mismatched IDs from the dataset\n",
    "mismatch_ids = collect_mismatch_ids(train_dataset, squad_raw['train'])\n",
    "\n",
    "# Remove the mismatches\n",
    "filtered_squad_raw = remove_mismatches(squad_raw['train'], mismatch_ids)\n",
    "\n",
    "print(\"Original dataset size:\", len(squad_raw['train']))\n",
    "print(\"Filtered dataset size:\", len(filtered_squad_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd97fb-7e0e-4b6d-88e4-30f1494d98ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Train BERT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a78faeab-a602-4b41-bfa6-ade9e4fcd7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='81590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  154/81590 00:46 < 6:52:52, 3.29 it/s, Epoch 0.02/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[227], line 42\u001b[0m\n\u001b[0;32m     11\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     12\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-finetuned-manual\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     overwrite_output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m QuestionAnsweringTrainer(\n\u001b[0;32m     30\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     31\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)],\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "# UNCOMMENT THIS TO TEST TRAINER BEFORE STARTING TUNING\n",
    "###############################################################\n",
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{pretrained_model_name}-finetuned-manual',\n",
    "    overwrite_output_dir = True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=4, \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",  # Enable logging to Weights & Biases\n",
    "    run_name=f\"{pretrained_model_name}-finetune-manual\",  # Optionally set a specific run name    \n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        eval_examples=eval_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5c3cd50-edfa-4b95-9e9d-b1c3271ec39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear cache if using GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "# Delete unnecessary variables\n",
    "del trainer\n",
    "del model\n",
    "del tokenizer\n",
    "del training_args\n",
    "del data_collator\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "# Clear TensorFlow session\n",
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c91d7-a210-4cf8-a811-df437554a8c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tune BERT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37d45d38-8ee1-482d-bea0-924718c093b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer  initialisation\n",
    "max_length = 870\n",
    "#global_doc_stride = 128\n",
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "data_collator = DefaultDataCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05916502-de91-4878-bf4c-7f579e0a88c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 04:07:29,827] A new study created in memory with name: no-name-db7b923a-638c-4277-9a20-68de3385eea1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 0 parameters: {'learning_rate': 5.272153980495028e-07, 'batch_size': 32, 'warmup_steps': 859, 'weight_decay': 0.04620569945443442, 'adam_beta1': 0.8256891086545727, 'adam_beta2': 0.9909432216963988, 'adam_epsilon': 1.4016712084494723e-07, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4079' max='40790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4079/40790 19:01 < 2:51:15, 3.57 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.482500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.505348</td>\n",
       "      <td>0.521765</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032881</td>\n",
       "      <td>5928</td>\n",
       "      <td>1.009251</td>\n",
       "      <td>1.009251</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23626cf47c0e44208dabac298fca24d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dcd6908bae4b8cae5d37eb93da5831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 04:27:40,604] Trial 0 finished with value: 0.5217654034296837 and parameters: {'learning_rate': 5.272153980495028e-07, 'batch_size': 32, 'warmup_steps': 859, 'weight_decay': 0.04620569945443442, 'adam_beta1': 0.8256891086545727, 'adam_beta2': 0.9909432216963988, 'adam_epsilon': 1.4016712084494723e-07, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 0.5217654034296837.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 1 parameters: {'learning_rate': 9.112562811696245e-07, 'batch_size': 32, 'warmup_steps': 675, 'weight_decay': 0.18797659052399981, 'adam_beta1': 0.8735254856491982, 'adam_beta2': 0.9931165383743351, 'adam_epsilon': 7.17813022711888e-07, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4079' max='40790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4079/40790 19:04 < 2:51:44, 3.56 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.272200</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.105618</td>\n",
       "      <td>2.123624</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036064</td>\n",
       "      <td>5928</td>\n",
       "      <td>4.205214</td>\n",
       "      <td>4.205214</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c585fd3f2da44dccbcf0038e4f22e75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bfed0234694acd9b12fd2d453e6c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 04:47:55,973] Trial 1 finished with value: 2.1236240642976636 and parameters: {'learning_rate': 9.112562811696245e-07, 'batch_size': 32, 'warmup_steps': 675, 'weight_decay': 0.18797659052399981, 'adam_beta1': 0.8735254856491982, 'adam_beta2': 0.9931165383743351, 'adam_epsilon': 7.17813022711888e-07, 'lr_scheduler_type': 'cosine'}. Best is trial 1 with value: 2.1236240642976636.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 2 parameters: {'learning_rate': 2.4352555091356716e-05, 'batch_size': 64, 'warmup_steps': 102, 'weight_decay': 0.0960175142746658, 'adam_beta1': 0.9203269672679533, 'adam_beta2': 0.9945796312670675, 'adam_epsilon': 3.870203562944912e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2040' max='20400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2040/20400 22:05 < 3:19:01, 1.54 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.182400</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.516045</td>\n",
       "      <td>1.559059</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086152</td>\n",
       "      <td>5928</td>\n",
       "      <td>3.027754</td>\n",
       "      <td>3.027754</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca9aa5a82ee4b758cb07dca5a09f6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a6b19f088444e6a6e90adb6b212684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 05:11:09,510] Trial 2 finished with value: 1.559059177729319 and parameters: {'learning_rate': 2.4352555091356716e-05, 'batch_size': 64, 'warmup_steps': 102, 'weight_decay': 0.0960175142746658, 'adam_beta1': 0.9203269672679533, 'adam_beta2': 0.9945796312670675, 'adam_epsilon': 3.870203562944912e-07, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 1 with value: 2.1236240642976636.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 3 parameters: {'learning_rate': 7.89205093341372e-06, 'batch_size': 16, 'warmup_steps': 643, 'weight_decay': 0.20096517412776863, 'adam_beta1': 0.9218089139543929, 'adam_beta2': 0.9901092398683391, 'adam_epsilon': 6.993165852578019e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8157' max='81570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8157/81570 20:27 < 3:04:12, 6.64 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.250300</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.326876</td>\n",
       "      <td>3.411402</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.033738</td>\n",
       "      <td>0.203032</td>\n",
       "      <td>5928</td>\n",
       "      <td>6.610597</td>\n",
       "      <td>6.610597</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bebc1f687bb4412aee05c211a4a9808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='749' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [749/749 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bce1dd88894d47b1eb476326c91e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 05:32:50,716] Trial 3 finished with value: 3.4114016195208823 and parameters: {'learning_rate': 7.89205093341372e-06, 'batch_size': 16, 'warmup_steps': 643, 'weight_decay': 0.20096517412776863, 'adam_beta1': 0.9218089139543929, 'adam_beta2': 0.9901092398683391, 'adam_epsilon': 6.993165852578019e-07, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 3 with value: 3.4114016195208823.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 4 parameters: {'learning_rate': 2.2469318749831854e-07, 'batch_size': 64, 'warmup_steps': 489, 'weight_decay': 0.08452794880633831, 'adam_beta1': 0.9067032984788023, 'adam_beta2': 0.9945792614320347, 'adam_epsilon': 1.6695932004785448e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4080' max='20400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4080/20400 41:51 < 2:47:32, 1.62 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.489500</td>\n",
       "      <td>No log</td>\n",
       "      <td>48.547124</td>\n",
       "      <td>48.598965</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103832</td>\n",
       "      <td>5928</td>\n",
       "      <td>96.955425</td>\n",
       "      <td>96.955425</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.280300</td>\n",
       "      <td>No log</td>\n",
       "      <td>15.985850</td>\n",
       "      <td>16.008897</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046159</td>\n",
       "      <td>5928</td>\n",
       "      <td>31.925988</td>\n",
       "      <td>31.925988</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdae11a82cf40cd92a4f50e4a7b6b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5966e3a9a14241699d92313c5e4258b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a0bece245948339113f0bcde05752a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 06:15:54,225] Trial 4 finished with value: 48.59896530008152 and parameters: {'learning_rate': 2.2469318749831854e-07, 'batch_size': 64, 'warmup_steps': 489, 'weight_decay': 0.08452794880633831, 'adam_beta1': 0.9067032984788023, 'adam_beta2': 0.9945792614320347, 'adam_epsilon': 1.6695932004785448e-07, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 5 parameters: {'learning_rate': 3.390860521175109e-05, 'batch_size': 32, 'warmup_steps': 19, 'weight_decay': 0.1345973564333013, 'adam_beta1': 0.9032935429231838, 'adam_beta2': 0.99127330075572, 'adam_epsilon': 7.330189861512721e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4079' max='40790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4079/40790 18:54 < 2:50:14, 3.59 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.070900</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.514445</td>\n",
       "      <td>4.542858</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056909</td>\n",
       "      <td>5928</td>\n",
       "      <td>9.015980</td>\n",
       "      <td>9.015980</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c27c61b521e4014a798d5a796009579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e50905853c6487a9a29e151f236cd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 06:35:56,939] Trial 5 finished with value: 4.54285829464807 and parameters: {'learning_rate': 3.390860521175109e-05, 'batch_size': 32, 'warmup_steps': 19, 'weight_decay': 0.1345973564333013, 'adam_beta1': 0.9032935429231838, 'adam_beta2': 0.99127330075572, 'adam_epsilon': 7.330189861512721e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 6 parameters: {'learning_rate': 1.4001217056971855e-05, 'batch_size': 32, 'warmup_steps': 102, 'weight_decay': 0.15275383707464713, 'adam_beta1': 0.9218124959734659, 'adam_beta2': 0.9947098748865553, 'adam_epsilon': 4.956921934218405e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4079' max='40790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4079/40790 19:01 < 2:51:14, 3.57 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.211400</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.886634</td>\n",
       "      <td>1.923697</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074233</td>\n",
       "      <td>5928</td>\n",
       "      <td>3.767872</td>\n",
       "      <td>3.767872</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed27fd69cec4f608d8617add2683d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ec775e3ab74d9ba2c2561a193f07be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 06:56:08,525] Trial 6 finished with value: 1.9236971298247254 and parameters: {'learning_rate': 1.4001217056971855e-05, 'batch_size': 32, 'warmup_steps': 102, 'weight_decay': 0.15275383707464713, 'adam_beta1': 0.9218124959734659, 'adam_beta2': 0.9947098748865553, 'adam_epsilon': 4.956921934218405e-07, 'lr_scheduler_type': 'linear'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 7 parameters: {'learning_rate': 4.0747556705130374e-05, 'batch_size': 64, 'warmup_steps': 161, 'weight_decay': 0.11929071635058283, 'adam_beta1': 0.8084574934077011, 'adam_beta2': 0.9957488444851792, 'adam_epsilon': 1.0984744229310127e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2040' max='20400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2040/20400 21:34 < 3:14:19, 1.57 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.115100</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.417249</td>\n",
       "      <td>2.460490</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.016869</td>\n",
       "      <td>0.103475</td>\n",
       "      <td>5928</td>\n",
       "      <td>4.810765</td>\n",
       "      <td>4.810765</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8a00d3efdc48fa8c4acfdea77364bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d46013aacf84cc6aa58535ed3d64601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 07:18:53,240] Trial 7 finished with value: 2.460490228956497 and parameters: {'learning_rate': 4.0747556705130374e-05, 'batch_size': 64, 'warmup_steps': 161, 'weight_decay': 0.11929071635058283, 'adam_beta1': 0.8084574934077011, 'adam_beta2': 0.9957488444851792, 'adam_epsilon': 1.0984744229310127e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 8 parameters: {'learning_rate': 3.1067114167729398e-06, 'batch_size': 64, 'warmup_steps': 724, 'weight_decay': 0.23659714447622004, 'adam_beta1': 0.934451903563282, 'adam_beta2': 0.9943891210647796, 'adam_epsilon': 4.96976324696024e-08, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2040' max='20400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2040/20400 18:25 < 2:45:55, 1.84 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.918300</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.009096</td>\n",
       "      <td>4.017519</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016869</td>\n",
       "      <td>5928</td>\n",
       "      <td>8.006728</td>\n",
       "      <td>8.006728</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f72f415a3d34d939387f4e35e550304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5902cab021ef4e5384ee37001364b53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 07:38:35,176] Trial 8 finished with value: 4.017518739998316 and parameters: {'learning_rate': 3.1067114167729398e-06, 'batch_size': 64, 'warmup_steps': 724, 'weight_decay': 0.23659714447622004, 'adam_beta1': 0.934451903563282, 'adam_beta2': 0.9943891210647796, 'adam_epsilon': 4.96976324696024e-08, 'lr_scheduler_type': 'linear'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 9 parameters: {'learning_rate': 4.804065400770467e-06, 'batch_size': 32, 'warmup_steps': 710, 'weight_decay': 0.15478119894492837, 'adam_beta1': 0.9459581122785004, 'adam_beta2': 0.9940963637057662, 'adam_epsilon': 1.0737522360526537e-07, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4079' max='40790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4079/40790 18:57 < 2:50:44, 3.58 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.479600</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.920323</td>\n",
       "      <td>1.933619</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026629</td>\n",
       "      <td>5928</td>\n",
       "      <td>3.835156</td>\n",
       "      <td>3.835156</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d212891cf31a454795a88717a30faf6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9b18c8434a4a16b0a3bd90e7b7086b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 07:58:42,892] Trial 9 finished with value: 1.9336188952124267 and parameters: {'learning_rate': 4.804065400770467e-06, 'batch_size': 32, 'warmup_steps': 710, 'weight_decay': 0.15478119894492837, 'adam_beta1': 0.9459581122785004, 'adam_beta2': 0.9940963637057662, 'adam_epsilon': 1.0737522360526537e-07, 'lr_scheduler_type': 'cosine'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 10 parameters: {'learning_rate': 1.298702067024549e-07, 'batch_size': 16, 'warmup_steps': 367, 'weight_decay': 0.018293601169950435, 'adam_beta1': 0.8624292103791529, 'adam_beta2': 0.997983352250956, 'adam_epsilon': 2.9457891098582263e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8157' max='81570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8157/81570 20:27 < 3:04:05, 6.65 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.564000</td>\n",
       "      <td>No log</td>\n",
       "      <td>36.839889</td>\n",
       "      <td>36.842003</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004234</td>\n",
       "      <td>5928</td>\n",
       "      <td>73.574432</td>\n",
       "      <td>73.574432</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6148f81f5bb94a79ab74de47f51cb330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='749' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [749/749 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c8c3503f664c3f882d9f13229fdafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 08:20:20,824] Trial 10 finished with value: 36.842002698493694 and parameters: {'learning_rate': 1.298702067024549e-07, 'batch_size': 16, 'warmup_steps': 367, 'weight_decay': 0.018293601169950435, 'adam_beta1': 0.8624292103791529, 'adam_beta2': 0.997983352250956, 'adam_epsilon': 2.9457891098582263e-07, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 11 parameters: {'learning_rate': 1.0426693752192976e-07, 'batch_size': 16, 'warmup_steps': 359, 'weight_decay': 0.024192948110454117, 'adam_beta1': 0.8712573879154358, 'adam_beta2': 0.9989486047162088, 'adam_epsilon': 3.153765729595085e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16314' max='81570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16314/81570 40:52 < 2:43:32, 6.65 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.828600</td>\n",
       "      <td>No log</td>\n",
       "      <td>48.462899</td>\n",
       "      <td>48.484903</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044071</td>\n",
       "      <td>5928</td>\n",
       "      <td>96.787216</td>\n",
       "      <td>96.787216</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.913800</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.275415</td>\n",
       "      <td>10.299989</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049218</td>\n",
       "      <td>5928</td>\n",
       "      <td>20.521447</td>\n",
       "      <td>20.521447</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68437f59b4734960a03b0213606cc506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8a29efa2eb4d51af39be34659d8150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='749' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [749/749 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee6c23247ae4d508d4f14ce166676d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 09:02:26,672] Trial 11 finished with value: 48.484902879252886 and parameters: {'learning_rate': 1.0426693752192976e-07, 'batch_size': 16, 'warmup_steps': 359, 'weight_decay': 0.024192948110454117, 'adam_beta1': 0.8712573879154358, 'adam_beta2': 0.9989486047162088, 'adam_epsilon': 3.153765729595085e-07, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 12 parameters: {'learning_rate': 1.0161866915957018e-07, 'batch_size': 16, 'warmup_steps': 388, 'weight_decay': 0.06895012241553194, 'adam_beta1': 0.8923644303318021, 'adam_beta2': 0.9984334700503453, 'adam_epsilon': 3.098798824923043e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8157' max='81570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8157/81570 20:10 < 3:01:38, 6.74 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.906300</td>\n",
       "      <td>No log</td>\n",
       "      <td>36.258738</td>\n",
       "      <td>36.532302</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547911</td>\n",
       "      <td>5928</td>\n",
       "      <td>72.413793</td>\n",
       "      <td>72.413793</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ecda38dd594f818d44fb2085c9fa9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='749' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [749/749 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76beae7bb5e64f3ba1bd56612ef58955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 09:23:35,564] Trial 12 finished with value: 36.53230170280029 and parameters: {'learning_rate': 1.0161866915957018e-07, 'batch_size': 16, 'warmup_steps': 388, 'weight_decay': 0.06895012241553194, 'adam_beta1': 0.8923644303318021, 'adam_beta2': 0.9984334700503453, 'adam_epsilon': 3.098798824923043e-07, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 13 parameters: {'learning_rate': 3.3598228160182775e-07, 'batch_size': 16, 'warmup_steps': 467, 'weight_decay': 0.013021018928619524, 'adam_beta1': 0.848630337128607, 'adam_beta2': 0.9964231717249393, 'adam_epsilon': 9.124019386670297e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8157' max='81570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8157/81570 20:05 < 3:00:50, 6.77 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.539100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.617114</td>\n",
       "      <td>1.629467</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024741</td>\n",
       "      <td>5928</td>\n",
       "      <td>3.229605</td>\n",
       "      <td>3.229605</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c40aef6e9f469da78d4d8f44c407b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='749' max='749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [749/749 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4771d9096ac8457ea0e195626b12dcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 09:44:39,398] Trial 13 finished with value: 1.6294674190740899 and parameters: {'learning_rate': 3.3598228160182775e-07, 'batch_size': 16, 'warmup_steps': 467, 'weight_decay': 0.013021018928619524, 'adam_beta1': 0.848630337128607, 'adam_beta2': 0.9964231717249393, 'adam_epsilon': 9.124019386670297e-07, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 4 with value: 48.59896530008152.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 14 parameters: {'learning_rate': 2.556070102851981e-07, 'batch_size': 64, 'warmup_steps': 286, 'weight_decay': 0.0666416550967841, 'adam_beta1': 0.8951450176750155, 'adam_beta2': 0.9965389756145857, 'adam_epsilon': 2.440104841737808e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4080' max='20400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4080/20400 42:06 < 2:48:30, 1.61 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.119800</td>\n",
       "      <td>No log</td>\n",
       "      <td>48.816643</td>\n",
       "      <td>48.854216</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075254</td>\n",
       "      <td>5928</td>\n",
       "      <td>97.493692</td>\n",
       "      <td>97.493692</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.986700</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.277015</td>\n",
       "      <td>7.293166</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032349</td>\n",
       "      <td>5928</td>\n",
       "      <td>14.533221</td>\n",
       "      <td>14.533221</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2e822f142c4b4da73fc7e677ce559a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e085428b1514f46bae86cbc1537960d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a469b15a6ca0429c8edb9fe91e3d787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 10:27:45,702] Trial 14 finished with value: 48.85421589869645 and parameters: {'learning_rate': 2.556070102851981e-07, 'batch_size': 64, 'warmup_steps': 286, 'weight_decay': 0.0666416550967841, 'adam_beta1': 0.8951450176750155, 'adam_beta2': 0.9965389756145857, 'adam_epsilon': 2.440104841737808e-07, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 14 with value: 48.85421589869645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 15 parameters: {'learning_rate': 1.2328081180397372e-06, 'batch_size': 64, 'warmup_steps': 302, 'weight_decay': 0.0817728855240299, 'adam_beta1': 0.8987522182815764, 'adam_beta2': 0.9968551818148843, 'adam_epsilon': 2.0844619532230343e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2040' max='20400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2040/20400 20:40 < 3:06:14, 1.64 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact</th>\n",
       "      <th>F1</th>\n",
       "      <th>Total</th>\n",
       "      <th>Hasans Exact</th>\n",
       "      <th>Hasans F1</th>\n",
       "      <th>Hasans Total</th>\n",
       "      <th>Noans Exact</th>\n",
       "      <th>Noans F1</th>\n",
       "      <th>Noans Total</th>\n",
       "      <th>Best Exact</th>\n",
       "      <th>Best Exact Thresh</th>\n",
       "      <th>Best F1</th>\n",
       "      <th>Best F1 Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.284100</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.417249</td>\n",
       "      <td>2.472294</td>\n",
       "      <td>11873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110247</td>\n",
       "      <td>5928</td>\n",
       "      <td>4.827586</td>\n",
       "      <td>4.827586</td>\n",
       "      <td>5945</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.071591</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddea21f73484588ac7d87099b1c2d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef212a26822e43c298498c2d996448cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_f1 below threshold of 45\n",
      "Stopping training: No improvement in eval_f1 for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 10:49:24,480] Trial 15 finished with value: 2.47229395907068 and parameters: {'learning_rate': 1.2328081180397372e-06, 'batch_size': 64, 'warmup_steps': 302, 'weight_decay': 0.0817728855240299, 'adam_beta1': 0.8987522182815764, 'adam_beta2': 0.9968551818148843, 'adam_epsilon': 2.0844619532230343e-07, 'lr_scheduler_type': 'constant_with_warmup'}. Best is trial 14 with value: 48.85421589869645.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 16 parameters: {'learning_rate': 3.023120268076598e-07, 'batch_size': 64, 'warmup_steps': 493, 'weight_decay': 0.05651478873758259, 'adam_beta1': 0.8914514746506399, 'adam_beta2': 0.9972158518028947, 'adam_epsilon': 4.696620940030543e-07, 'lr_scheduler_type': 'constant_with_warmup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='20400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   82/20400 00:48 < 3:25:36, 1.65 it/s, Epoch 0.04/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-05-25 10:50:16,045] Trial 16 failed with parameters: {'learning_rate': 3.023120268076598e-07, 'batch_size': 64, 'warmup_steps': 493, 'weight_decay': 0.05651478873758259, 'adam_beta1': 0.8914514746506399, 'adam_beta2': 0.9972158518028947, 'adam_epsilon': 4.696620940030543e-07, 'lr_scheduler_type': 'constant_with_warmup'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\OEM\\AppData\\Local\\Temp\\ipykernel_1644\\3488163911.py\", line 83, in objective\n",
      "    trainer.train()\n",
      "  File \"C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 1885, in train\n",
      "    return inner_training_loop(\n",
      "  File \"C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 2221, in _inner_training_loop\n",
      "    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
      "KeyboardInterrupt\n",
      "[W 2024-05-25 10:50:16,046] Trial 16 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a study object and optimize the objective\u001b[39;00m\n\u001b[0;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[12], line 83\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     67\u001b[0m trainer \u001b[38;5;241m=\u001b[39m QuestionAnsweringTrainer(\n\u001b[0;32m     68\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_init(),\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[AdvancedEarlyStoppingCallback(metric_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m)]\n\u001b[0;32m     79\u001b[0m )  \n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     86\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a study object and optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b73e5-5114-4c5e-b297-2968fcf03853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache if using GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "# Delete unnecessary variables\n",
    "del data_collator\n",
    "del tokenizer\n",
    "del study\n",
    "\n",
    "# Collect garbage\n",
    "gc.collect()\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "# Clear TensorFlow session\n",
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74542448-aaae-4dff-8b32-93162e5e8589",
   "metadata": {},
   "source": [
    "## 4. Tune RoBERTa Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1f572-cce3-4dc1-861e-26f59a8c45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer  initialisation\n",
    "max_length = 870\n",
    "#global_doc_stride = 128\n",
    "pretrained_model_name = \"bert-base-uncased\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "data_collator = DefaultDataCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e3e06b-1c61-4ea7-a77d-48fcb7f9e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object and optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38836682-b31a-4252-bcb0-6e4028b5b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0296ed7-9e1f-49d0-8623-0ecc4cd410eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Tune XLNet Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec1f961-b82a-473c-8675-4a8d749c3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer  initialisation\n",
    "max_length = 870\n",
    "#global_doc_stride = 128\n",
    "pretrained_model_name = \"xlnet/xlnet-base-cased\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "data_collator = DefaultDataCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafadcfc-3c04-4210-be0a-eeb626d0bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object and optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab3ce3-8c6e-43b7-be84-5416a2e3c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d81a2-0537-4c8d-9a5a-4fd4c9bcdf71",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Tune Facebook RoBERTa Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1231c5d-1800-41b5-93ae-32edbf4d49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenizer  initialisation\n",
    "max_length = 870\n",
    "#global_doc_stride = 128\n",
    "pretrained_model_name = \"FacebookAI/roberta-base\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "data_collator = DefaultDataCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f0817-abf5-4dbb-b32b-947f4f3010b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study object and optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8071c-b21f-4c0e-ba64-85b7e5613f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2726fa-3e3a-4eb6-a700-b57cd6d69ccf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Tune AlBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3511b-2dc7-4be3-b63c-06e7f019cad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tokenizer  initialisation\n",
    "max_length = 870\n",
    "#global_doc_stride = 128\n",
    "pretrained_model_name = \"albert/albert-base-v2\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "right_padding = tokenizer.padding_side == 'right'\n",
    "data_collator = DefaultDataCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2cb00-961d-4af3-be1a-0ed1534eb978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a study object and optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f7a66-27da-41d4-bcc0-bbd110efaab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsci714win",
   "language": "python",
   "name": "compsci714win"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
