{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\miniconda3\\envs\\pytorch-modified\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "#from tqdm.auto import tqdm\n",
    "\n",
    "# misc\n",
    "import re\n",
    "import time\n",
    "import ast\n",
    "import warnings\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "# from xgboost import plot_importance\n",
    "# import seaborn as sns\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from torch.utils.data import random_split\n",
    "import datasets\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "#import scikitplot as skplt\n",
    "# import xgboost\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import DefaultDataCollator, EvalPrediction\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, XLMRobertaForSequenceClassification\n",
    "from transformers import DistilBertConfig, DistilBertModel, BertTokenizerFast\n",
    "from transformers import AutoModel\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, BertForQuestionAnswering, AutoModelForQuestionAnswering, DistilBertForSequenceClassification, DistilBertModel, CamembertForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from transformers import TrainerCallback, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_raw = load_dataset(\"squad_v2\", split=\"train[:100]+validation[:20]\")\n",
    "train_testvalid = squad_raw.train_test_split(test_size=0.4, shuffle=True)\n",
    "\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, shuffle=True)\n",
    "\n",
    "squad_raw = datasets.DatasetDict({\n",
    "                                'train': train_testvalid['train'],\n",
    "                                'validation': test_valid['train'],\n",
    "                                'test': test_valid['test']\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_positions(dataset):\n",
    "    dataset_answers = dataset['answers']\n",
    "    dataset_contexts = dataset['context']\n",
    "    updated_answers = []\n",
    "\n",
    "    for i, (answer, text) in enumerate(zip(dataset_answers, dataset_contexts)):\n",
    "        if len(answer['answer_start']) == 0:\n",
    "            answer['answer_end'] = []\n",
    "            updated_answers.append(answer)\n",
    "            continue\n",
    "        \n",
    "        real_answer = answer['text'][0]\n",
    "        real_answer_length = len(real_answer)\n",
    "        start_idx = answer['answer_start'][0]\n",
    "        end_idx = start_idx + real_answer_length\n",
    "\n",
    "        if text[start_idx:end_idx] == real_answer:\n",
    "            answer['answer_end'] = [end_idx]\n",
    "        elif text[start_idx-1:end_idx-1] == real_answer:\n",
    "            answer['answer_start'] = [start_idx-1]\n",
    "            answer['answer_end'] = [end_idx-1]\n",
    "        elif text[start_idx-2:end_idx-2] == real_answer:\n",
    "            answer['answer_start'] = [start_idx-2]\n",
    "            answer['answer_end'] = [end_idx-2]\n",
    "        updated_answers.append(answer)\n",
    "\n",
    "    for answer in updated_answers:\n",
    "        if 'answer_end' not in answer:\n",
    "            answer['answer_end'] = []\n",
    "            \n",
    "    updated_dataset = {\n",
    "        'id': dataset['id'],\n",
    "        'title': dataset['title'],\n",
    "        'context': dataset['context'],\n",
    "        'question': dataset['question'],\n",
    "        'answers': updated_answers\n",
    "    }\n",
    "\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_raw_train = find_positions(squad_raw[\"train\"])\n",
    "squad_raw_val = find_positions(squad_raw[\"validation\"])\n",
    "squad_raw_test = find_positions(squad_raw[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\miniconda3\\envs\\pytorch-modified\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_texts = squad_raw_train[\"context\"]\n",
    "train_queries = squad_raw_train[\"question\"]\n",
    "val_texts = squad_raw_val[\"context\"]\n",
    "val_queries = squad_raw_val[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, train_queries, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, val_queries, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "  start_positions = []\n",
    "  end_positions = []\n",
    "  count = 0\n",
    "  \n",
    "  for i in range(len(answers)):\n",
    "    temp_start = None\n",
    "    temp_end = None\n",
    "    \n",
    "    if len(answers[i]['answer_start']) != 0:\n",
    "      temp_start = answers[i]['answer_start'][0]\n",
    "      temp_end = answers[i]['answer_end'][0]\n",
    "\n",
    "    start_positions.append(encodings.char_to_token(i, temp_start))\n",
    "    end_positions.append(encodings.char_to_token(i, temp_end))\n",
    "    \n",
    "    if start_positions[-1] is None:\n",
    "      start_positions[-1] = tokenizer.model_max_length\n",
    "      \n",
    "    if end_positions[-1] is None:\n",
    "      if end_positions[-1] is not None:\n",
    "        end_positions[-1] = encodings.char_to_token(i, temp_end + 1)\n",
    "      if end_positions[-1] is None:\n",
    "        count += 1\n",
    "        end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "  print(count)\n",
    "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, squad_raw_train[\"answers\"])\n",
    "add_token_positions(val_encodings, squad_raw_val[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\envs\\pytorch-modified\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:266\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'to_dict'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m train_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m train_examples:\n\u001b[1;32m----> 6\u001b[0m   train_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m()\u001b[38;5;241m.\u001b[39mvalues()) \n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_data\u001b[39m(examples, batch_size):\n\u001b[0;32m      9\u001b[0m   batched \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\envs\\pytorch-modified\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:268\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_examples = [train_encodings]\n",
    "\n",
    "# Convert to list\n",
    "train_list = []\n",
    "for sample in train_examples:\n",
    "  train_list.extend(sample.to_dict().values()) \n",
    "\n",
    "def batch_data(examples, batch_size):\n",
    "  batched = {}\n",
    "  \n",
    "  for key in examples[0]:\n",
    "    batched[key] = [sample[key] for sample in examples[:batch_size]]\n",
    "\n",
    "  return batched\n",
    "\n",
    "train_batches = []\n",
    "\n",
    "for i in range(0, len(train_list), batch_size):\n",
    "  batch = train_list[i:i+batch_size]\n",
    "  train_batches.append(batch_data(batch, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SquadDataset' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SquadDataset(train_encodings)\n\u001b[0;32m     20\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m SquadDataset(val_encodings)\n\u001b[1;32m---> 22\u001b[0m batched_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(batch_dataset, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     23\u001b[0m batched_val_dataset \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mmap(batch_dataset, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SquadDataset' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "def batch_dataset(dataset):\n",
    "  batched = {}\n",
    "  for key in dataset[0]:\n",
    "    batched[key] = [ sample[key] for sample in dataset]\n",
    "  return batched\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)\n",
    "\n",
    "batched_train_dataset = train_dataset.map(batch_dataset, batched=True, batch_size=batch_size)\n",
    "batched_val_dataset = val_dataset.map(batch_dataset, batched=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dataset(examples):\n",
    "  batched_examples = {\"input_ids\": torch.tensor([x[\"input_ids\"] for x in examples])}\n",
    "\n",
    "  return batched_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 21:34:58,286 - DEBUG - https://huggingface.co:443 \"HEAD /bert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/3 [01:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: training requires a train_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain_dataloader \u001b[38;5;241m=\u001b[39m train_loader\n\u001b[0;32m     22\u001b[0m trainer\u001b[38;5;241m.\u001b[39meval_dataloader \u001b[38;5;241m=\u001b[39m val_loader\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\envs\\pytorch-modified\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\envs\\pytorch-modified\\Lib\\site-packages\\transformers\\trainer.py:1568\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1566\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1567\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[1;32m-> 1568\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;66;03m# Setting up training control variables:\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;66;03m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;66;03m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;66;03m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m total_train_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m*\u001b[39m args\u001b[38;5;241m.\u001b[39mworld_size\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\envs\\pytorch-modified\\Lib\\site-packages\\transformers\\trainer.py:788\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;124;03mReturns the training [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[0;32m    781\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;124;03mSubclass and override this method if you want to inject some custom behavior.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 788\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: training requires a train_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    790\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset\n\u001b[0;32m    791\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n",
      "\u001b[1;31mValueError\u001b[0m: Trainer: training requires a train_dataset."
     ]
    }
   ],
   "source": [
    "data_collator = DefaultDataCollator()\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"squadv2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train_dataloader = train_loader\n",
    "trainer.eval_dataloader = val_loader\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
