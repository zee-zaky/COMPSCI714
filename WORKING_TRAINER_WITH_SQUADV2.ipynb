{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07a3fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 2060 is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "#from tqdm.auto import tqdm\n",
    "\n",
    "# misc\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import ast\n",
    "import warnings\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "import seaborn as sns\n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from torch.utils.data import random_split\n",
    "import datasets\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "#import scikitplot as skplt\n",
    "import xgboost\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from transformers import DefaultDataCollator, EvalPrediction\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, XLMRobertaForSequenceClassification\n",
    "from transformers import DistilBertConfig, DistilBertModel\n",
    "from transformers import AutoModel\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoModelForQuestionAnswering, DistilBertForSequenceClassification, DistilBertModel, CamembertForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from transformers import TrainerCallback, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import torch\n",
    "import evaluate\n",
    "import optuna\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")\n",
    "\n",
    "### Compute_metrics function for Question and Answering problem is different to classification, more preocessing required.\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "        return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba211d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A subclass of `Trainer` specific to Question-Answering tasks\n",
    "\"\"\"\n",
    "import math\n",
    "import time\n",
    "\n",
    "from transformers import Trainer, is_torch_xla_available\n",
    "from transformers.trainer_utils import PredictionOutput, speed_metrics\n",
    "\n",
    "\n",
    "if is_torch_xla_available():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.debug.metrics as met\n",
    "\n",
    "\n",
    "class QuestionAnsweringTrainer(Trainer):\n",
    "    def __init__(self, *args, eval_examples=None, post_process_function=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_examples = eval_examples\n",
    "        self.post_process_function = post_process_function\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
    "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        eval_examples = self.eval_examples if eval_examples is None else eval_examples\n",
    "\n",
    "        # Temporarily disable metric computation, we will do it in the loop here.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            output = eval_loop(\n",
    "                eval_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "                metric_key_prefix=metric_key_prefix,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=output.num_samples,\n",
    "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "        if self.post_process_function is not None and self.compute_metrics is not None and self.args.should_save:\n",
    "            # Only the main node write the results by default\n",
    "            eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)\n",
    "            metrics = self.compute_metrics(eval_preds)\n",
    "\n",
    "            # Prefix all keys with metric_key_prefix + '_'\n",
    "            for key in list(metrics.keys()):\n",
    "                if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                    metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "            metrics.update(output.metrics)\n",
    "        else:\n",
    "            metrics = output.metrics\n",
    "\n",
    "        if self.args.should_log:\n",
    "            # Only the main node log the results by default\n",
    "            self.log(metrics)\n",
    "\n",
    "        if self.args.tpu_metrics_debug or self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n",
    "        return metrics\n",
    "\n",
    "    def predict(self, predict_dataset, predict_examples, ignore_keys=None, metric_key_prefix: str = \"test\"):\n",
    "        predict_dataloader = self.get_test_dataloader(predict_dataset)\n",
    "\n",
    "        # Temporarily disable metric computation, we will do it in the loop here.\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            output = eval_loop(\n",
    "                predict_dataloader,\n",
    "                description=\"Prediction\",\n",
    "                # No point gathering the predictions if there are no metrics, otherwise we defer to\n",
    "                # self.args.prediction_loss_only\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "                metric_key_prefix=metric_key_prefix,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "        total_batch_size = self.args.eval_batch_size * self.args.world_size\n",
    "        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n",
    "            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n",
    "        output.metrics.update(\n",
    "            speed_metrics(\n",
    "                metric_key_prefix,\n",
    "                start_time,\n",
    "                num_samples=output.num_samples,\n",
    "                num_steps=math.ceil(output.num_samples / total_batch_size),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if self.post_process_function is None or self.compute_metrics is None:\n",
    "            return output\n",
    "\n",
    "        predictions = self.post_process_function(predict_examples, predict_dataset, output.predictions, \"predict\")\n",
    "        metrics = self.compute_metrics(predictions)\n",
    "\n",
    "        # Prefix all keys with metric_key_prefix + '_'\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "        metrics.update(output.metrics)\n",
    "        return PredictionOutput(predictions=predictions.predictions, label_ids=predictions.label_ids, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06255249",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RECORDS = 200\n",
    "\n",
    "\n",
    "with open('./train-v2.0.json', 'rb') as f:\n",
    "    squad = json.load(f)\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "for group in squad['data']:\n",
    "    for parag in group['paragraphs']:\n",
    "        context = parag['context']\n",
    "        for qa in parag['qas']:\n",
    "            question = qa['question']\n",
    "            for answer in qa['answers']:\n",
    "                contexts.append(context)\n",
    "                questions.append(question)\n",
    "                answers.append(answer)\n",
    "\n",
    "contexts, questions, answers = contexts[:NUM_RECORDS], questions[:NUM_RECORDS], answers[:NUM_RECORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7eda243",
   "metadata": {},
   "outputs": [],
   "source": [
    "for answer, context in zip(answers, contexts):\n",
    "    gold_text = answer[\"text\"]\n",
    "    start_idx = answer[\"answer_start\"]\n",
    "\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "\n",
    "    if context[start_idx:end_idx] == gold_text:\n",
    "        answer[\"answer_end\"] = end_idx\n",
    "    elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "        answer[\"answer_start\"] = start_idx - 1\n",
    "        answer[\"answer_end\"] = end_idx - 1\n",
    "    elif context[start_idx-1:end_idx-2] == gold_text:\n",
    "        answer[\"answer_start\"] = start_idx - 2\n",
    "        answer[\"answer_end\"] = end_idx - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f54bd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'in the late 1990s', 'answer_start': 269, 'answer_end': 286},\n",
       " {'text': 'singing and dancing', 'answer_start': 207, 'answer_end': 226},\n",
       " {'text': '2003', 'answer_start': 526, 'answer_end': 530},\n",
       " {'text': 'Houston, Texas', 'answer_start': 166, 'answer_end': 180},\n",
       " {'text': 'late 1990s', 'answer_start': 276, 'answer_end': 286},\n",
       " {'text': \"Destiny's Child\", 'answer_start': 320, 'answer_end': 335},\n",
       " {'text': 'Dangerously in Love', 'answer_start': 505, 'answer_end': 524},\n",
       " {'text': 'Mathew Knowles', 'answer_start': 360, 'answer_end': 374},\n",
       " {'text': 'late 1990s', 'answer_start': 276, 'answer_end': 286},\n",
       " {'text': 'lead singer', 'answer_start': 290, 'answer_end': 301},\n",
       " {'text': 'Dangerously in Love', 'answer_start': 505, 'answer_end': 524},\n",
       " {'text': '2003', 'answer_start': 526, 'answer_end': 530},\n",
       " {'text': 'five', 'answer_start': 590, 'answer_end': 594},\n",
       " {'text': 'lead singer', 'answer_start': 290, 'answer_end': 301},\n",
       " {'text': 'Dangerously in Love', 'answer_start': 505, 'answer_end': 524},\n",
       " {'text': 'acting', 'answer_start': 207, 'answer_end': 213},\n",
       " {'text': 'Jay Z', 'answer_start': 369, 'answer_end': 374},\n",
       " {'text': 'six', 'answer_start': 565, 'answer_end': 568},\n",
       " {'text': 'Dreamgirls', 'answer_start': 260, 'answer_end': 270},\n",
       " {'text': '2010', 'answer_start': 586, 'answer_end': 590},\n",
       " {'text': 'Beyoncé', 'answer_start': 180, 'answer_end': 187},\n",
       " {'text': 'Cadillac Records', 'answer_start': 406, 'answer_end': 422},\n",
       " {'text': 'June 2005', 'answer_start': 48, 'answer_end': 57},\n",
       " {'text': \"B'Day\", 'answer_start': 95, 'answer_end': 100},\n",
       " {'text': 'Dreamgirls', 'answer_start': 260, 'answer_end': 270},\n",
       " {'text': 'Jay Z', 'answer_start': 369, 'answer_end': 374},\n",
       " {'text': 'Sasha Fierce', 'answer_start': 466, 'answer_end': 478},\n",
       " {'text': 'love, relationships, and monogamy',\n",
       "  'answer_start': 104,\n",
       "  'answer_end': 137},\n",
       " {'text': 'influential', 'answer_start': 935, 'answer_end': 946},\n",
       " {'text': 'Forbes', 'answer_start': 985, 'answer_end': 991},\n",
       " {'text': '2000s', 'answer_start': 736, 'answer_end': 741},\n",
       " {'text': 'Forbes', 'answer_start': 985, 'answer_end': 991},\n",
       " {'text': 'modern-day feminist', 'answer_start': 18, 'answer_end': 37},\n",
       " {'text': '2013 and 2014', 'answer_start': 970, 'answer_end': 983},\n",
       " {'text': '118 million', 'answer_start': 393, 'answer_end': 404},\n",
       " {'text': '60 million', 'answer_start': 445, 'answer_end': 455},\n",
       " {'text': '118 million', 'answer_start': 393, 'answer_end': 404},\n",
       " {'text': '20', 'answer_start': 552, 'answer_end': 554},\n",
       " {'text': 'Forbes', 'answer_start': 985, 'answer_end': 991},\n",
       " {'text': \"Destiny's Child\", 'answer_start': 303, 'answer_end': 318},\n",
       " {'text': \"her mother's maiden name\", 'answer_start': 204, 'answer_end': 228},\n",
       " {'text': 'African-American', 'answer_start': 330, 'answer_end': 346},\n",
       " {'text': 'Methodist', 'answer_start': 578, 'answer_end': 587},\n",
       " {'text': 'Xerox', 'answer_start': 152, 'answer_end': 157},\n",
       " {'text': 'hairdresser and salon owner',\n",
       "  'answer_start': 101,\n",
       "  'answer_end': 128},\n",
       " {'text': 'Solange', 'answer_start': 255, 'answer_end': 262},\n",
       " {'text': 'Joseph Broussard', 'answer_start': 540, 'answer_end': 556},\n",
       " {'text': 'Xerox', 'answer_start': 152, 'answer_end': 157},\n",
       " {'text': 'salon', 'answer_start': 117, 'answer_end': 122},\n",
       " {'text': 'Solange', 'answer_start': 255, 'answer_end': 262},\n",
       " {'text': 'Joseph Broussard.', 'answer_start': 540, 'answer_end': 557},\n",
       " {'text': 'Methodist', 'answer_start': 578, 'answer_end': 587},\n",
       " {'text': 'Fredericksburg', 'answer_start': 49, 'answer_end': 63},\n",
       " {'text': 'Darlette Johnson', 'answer_start': 165, 'answer_end': 181},\n",
       " {'text': 'Houston', 'answer_start': 507, 'answer_end': 514},\n",
       " {'text': 'dance instructor Darlette Johnson',\n",
       "  'answer_start': 148,\n",
       "  'answer_end': 181},\n",
       " {'text': \"St. John's United Methodist Church\",\n",
       "  'answer_start': 711,\n",
       "  'answer_end': 745},\n",
       " {'text': 'music magnet school', 'answer_start': 484, 'answer_end': 503},\n",
       " {'text': 'Imagine', 'answer_start': 385, 'answer_end': 392},\n",
       " {'text': 'Fredericksburg', 'answer_start': 49, 'answer_end': 63},\n",
       " {'text': 'Darlette Johnson', 'answer_start': 165, 'answer_end': 181},\n",
       " {'text': 'seven', 'answer_start': 355, 'answer_end': 360},\n",
       " {'text': \"St. John's United Methodist Church\",\n",
       "  'answer_start': 711,\n",
       "  'answer_end': 745},\n",
       " {'text': 'Arne Frager', 'answer_start': 303, 'answer_end': 314},\n",
       " {'text': \"Beyoncé's father\", 'answer_start': 542, 'answer_end': 558},\n",
       " {'text': 'Elektra Records', 'answer_start': 918, 'answer_end': 933},\n",
       " {'text': 'Arne Frager', 'answer_start': 303, 'answer_end': 314},\n",
       " {'text': '1995', 'answer_start': 537, 'answer_end': 541},\n",
       " {'text': 'Sony Music', 'answer_start': 1264, 'answer_end': 1274},\n",
       " {'text': 'Elektra Records', 'answer_start': 918, 'answer_end': 933},\n",
       " {'text': 'age eight', 'answer_start': 3, 'answer_end': 12},\n",
       " {'text': 'eight', 'answer_start': 7, 'answer_end': 12},\n",
       " {'text': \"Girl's Tyme\", 'answer_start': 192, 'answer_end': 203},\n",
       " {'text': 'Arne Frager', 'answer_start': 303, 'answer_end': 314},\n",
       " {'text': '1995', 'answer_start': 537, 'answer_end': 541},\n",
       " {'text': \"Dwayne Wiggins's Grass Roots Entertainment\",\n",
       "  'answer_start': 1126,\n",
       "  'answer_end': 1168},\n",
       " {'text': 'Men in Black', 'answer_start': 215, 'answer_end': 227},\n",
       " {'text': '\"Say My Name\"', 'answer_start': 848, 'answer_end': 861},\n",
       " {'text': 'Marc Nelson', 'answer_start': 1212, 'answer_end': 1223},\n",
       " {'text': '1996', 'answer_start': 51, 'answer_end': 55},\n",
       " {'text': 'Book of Isaiah', 'answer_start': 85, 'answer_end': 99},\n",
       " {'text': 'Men in Black', 'answer_start': 215, 'answer_end': 227},\n",
       " {'text': 'Say My Name', 'answer_start': 849, 'answer_end': 860},\n",
       " {'text': 'Marc Nelson', 'answer_start': 1212, 'answer_end': 1223},\n",
       " {'text': 'Book of Isaiah.', 'answer_start': 85, 'answer_end': 100},\n",
       " {'text': 'Men in Black.', 'answer_start': 215, 'answer_end': 228},\n",
       " {'text': 'No, No, No', 'answer_start': 330, 'answer_end': 340},\n",
       " {'text': '1999', 'answer_start': 688, 'answer_end': 692},\n",
       " {'text': 'Marc Nelson', 'answer_start': 1212, 'answer_end': 1223},\n",
       " {'text': 'depression', 'answer_start': 169, 'answer_end': 179},\n",
       " {'text': 'boyfriend left her', 'answer_start': 320, 'answer_end': 338},\n",
       " {'text': 'her mother', 'answer_start': 714, 'answer_end': 724},\n",
       " {'text': 'split with Luckett and Rober',\n",
       "  'answer_start': 194,\n",
       "  'answer_end': 222},\n",
       " {'text': 'a couple of years', 'answer_start': 396, 'answer_end': 413},\n",
       " {'text': 'her mother', 'answer_start': 714, 'answer_end': 724},\n",
       " {'text': 'Farrah Franklin and Michelle Williams.',\n",
       "  'answer_start': 110,\n",
       "  'answer_end': 148},\n",
       " {'text': 'Beyoncé', 'answer_start': 149, 'answer_end': 156},\n",
       " {'text': 'her mother', 'answer_start': 714, 'answer_end': 724},\n",
       " {'text': 'Farrah Franklin', 'answer_start': 110, 'answer_end': 125},\n",
       " {'text': 'Independent Women Part I', 'answer_start': 37, 'answer_end': 61},\n",
       " {'text': 'eleven', 'answer_start': 216, 'answer_end': 222},\n",
       " {'text': 'MTV', 'answer_start': 348, 'answer_end': 351},\n",
       " {'text': '663,000 copies', 'answer_start': 793, 'answer_end': 807},\n",
       " {'text': 'Georges Bizet', 'answer_start': 557, 'answer_end': 570},\n",
       " {'text': 'Survivor', 'answer_start': 593, 'answer_end': 601},\n",
       " {'text': \"Charlie's Angels.\", 'answer_start': 115, 'answer_end': 132},\n",
       " {'text': 'Carmen: A Hip Hopera', 'answer_start': 378, 'answer_end': 398},\n",
       " {'text': 'Survivor', 'answer_start': 593, 'answer_end': 601},\n",
       " {'text': 'Luckett and Roberson', 'answer_start': 628, 'answer_end': 648},\n",
       " {'text': 'October 2001', 'answer_start': 1070, 'answer_end': 1082},\n",
       " {'text': 'Mike Myers', 'answer_start': 84, 'answer_end': 94},\n",
       " {'text': 'UK, Norway, and Belgium', 'answer_start': 331, 'answer_end': 354},\n",
       " {'text': 'The Fighting Temptations', 'answer_start': 431, 'answer_end': 455},\n",
       " {'text': 'Missy Elliott', 'answer_start': 705, 'answer_end': 718},\n",
       " {'text': 'Summertime', 'answer_start': 834, 'answer_end': 844},\n",
       " {'text': 'Austin Powers in Goldmember',\n",
       "  'answer_start': 115,\n",
       "  'answer_end': 142},\n",
       " {'text': '73 million', 'answer_start': 210, 'answer_end': 220},\n",
       " {'text': 'musical comedy', 'answer_start': 416, 'answer_end': 430},\n",
       " {'text': 'Fighting Temptations', 'answer_start': 435, 'answer_end': 455},\n",
       " {'text': 'mixed reviews', 'answer_start': 545, 'answer_end': 558},\n",
       " {'text': 'Austin Powers in Goldmember',\n",
       "  'answer_start': 115,\n",
       "  'answer_end': 142},\n",
       " {'text': 'Foxxy Cleopatra', 'answer_start': 58, 'answer_end': 73},\n",
       " {'text': 'Work It Out', 'answer_start': 240, 'answer_end': 251},\n",
       " {'text': 'The Fighting Temptations', 'answer_start': 431, 'answer_end': 455},\n",
       " {'text': 'Fighting Temptations', 'answer_start': 435, 'answer_end': 455},\n",
       " {'text': 'number four', 'answer_start': 123, 'answer_end': 134},\n",
       " {'text': 'Dangerously in Love', 'answer_start': 193, 'answer_end': 212},\n",
       " {'text': '11 million', 'answer_start': 419, 'answer_end': 429},\n",
       " {'text': 'Crazy in Love', 'answer_start': 474, 'answer_end': 487},\n",
       " {'text': 'four', 'answer_start': 130, 'answer_end': 134},\n",
       " {'text': 'Jay Z', 'answer_start': 48, 'answer_end': 53},\n",
       " {'text': 'Dangerously in Love', 'answer_start': 193, 'answer_end': 212},\n",
       " {'text': 'number four', 'answer_start': 123, 'answer_end': 134},\n",
       " {'text': 'Luther Vandross', 'answer_start': 1042, 'answer_end': 1057},\n",
       " {'text': 'Jay Z', 'answer_start': 48, 'answer_end': 53},\n",
       " {'text': 'June 24, 2003', 'answer_start': 229, 'answer_end': 242},\n",
       " {'text': 'Crazy in Love', 'answer_start': 474, 'answer_end': 487},\n",
       " {'text': 'Luther Vandross.', 'answer_start': 1042, 'answer_end': 1058},\n",
       " {'text': 'five.', 'answer_start': 696, 'answer_end': 701},\n",
       " {'text': 'Destiny Fulfilled', 'answer_start': 513, 'answer_end': 530},\n",
       " {'text': '2006', 'answer_start': 1212, 'answer_end': 1216},\n",
       " {'text': 'November 2003', 'answer_start': 3, 'answer_end': 16},\n",
       " {'text': 'Destiny Fulfilled', 'answer_start': 664, 'answer_end': 681},\n",
       " {'text': 'Barcelona', 'answer_start': 935, 'answer_end': 944},\n",
       " {'text': 'March 2006', 'answer_start': 1206, 'answer_end': 1216},\n",
       " {'text': 'Dangerously in Love Tour', 'answer_start': 38, 'answer_end': 62},\n",
       " {'text': 'Missy Elliott and Alicia Keys',\n",
       "  'answer_start': 100,\n",
       "  'answer_end': 129},\n",
       " {'text': 'Super Bowl XXXVIII', 'answer_start': 253, 'answer_end': 271},\n",
       " {'text': 'Destiny Fulfilled.', 'answer_start': 848, 'answer_end': 866},\n",
       " {'text': '541,000', 'answer_start': 132, 'answer_end': 139},\n",
       " {'text': 'Déjà Vu', 'answer_start': 303, 'answer_end': 310},\n",
       " {'text': 'five', 'answer_start': 346, 'answer_end': 350},\n",
       " {'text': 'five', 'answer_start': 346, 'answer_end': 350},\n",
       " {'text': 'twenty-fifth birthday', 'answer_start': 101, 'answer_end': 122},\n",
       " {'text': 'Jay Z', 'answer_start': 323, 'answer_end': 328},\n",
       " {'text': 'top five', 'answer_start': 342, 'answer_end': 350},\n",
       " {'text': \"B'Day\", 'answer_start': 28, 'answer_end': 33},\n",
       " {'text': '541,000', 'answer_start': 132, 'answer_end': 139},\n",
       " {'text': 'Jay Z', 'answer_start': 323, 'answer_end': 328},\n",
       " {'text': 'Green Light', 'answer_start': 635, 'answer_end': 646},\n",
       " {'text': 'The Pink Panther', 'answer_start': 53, 'answer_end': 69},\n",
       " {'text': 'Dreamgirls', 'answer_start': 171, 'answer_end': 181},\n",
       " {'text': 'Dreamgirls', 'answer_start': 171, 'answer_end': 181},\n",
       " {'text': '2007', 'answer_start': 550, 'answer_end': 554},\n",
       " {'text': '24 million', 'answer_start': 671, 'answer_end': 681},\n",
       " {'text': '158.8 million', 'answer_start': 112, 'answer_end': 125},\n",
       " {'text': 'The Beyoncé Experience', 'answer_start': 576, 'answer_end': 598},\n",
       " {'text': 'Shakira', 'answer_start': 932, 'answer_end': 939},\n",
       " {'text': 'The Pink Panther', 'answer_start': 53, 'answer_end': 69},\n",
       " {'text': 'Diana Ross.', 'answer_start': 436, 'answer_end': 447},\n",
       " {'text': 'Listen', 'answer_start': 487, 'answer_end': 493},\n",
       " {'text': 'The Beyoncé Experience', 'answer_start': 576, 'answer_end': 598},\n",
       " {'text': 'Shakira', 'answer_start': 932, 'answer_end': 939},\n",
       " {'text': 'Jay Z', 'answer_start': 34, 'answer_end': 39},\n",
       " {'text': 'November 18, 2008', 'answer_start': 253, 'answer_end': 270},\n",
       " {'text': '2000s', 'answer_start': 897, 'answer_end': 902},\n",
       " {'text': 'Taylor Swift', 'answer_start': 1567, 'answer_end': 1579},\n",
       " {'text': '119.5 million', 'answer_start': 1881, 'answer_end': 1894},\n",
       " {'text': 'in a video montage', 'answer_start': 78, 'answer_end': 96},\n",
       " {'text': 'March 2009', 'answer_start': 1744, 'answer_end': 1754},\n",
       " {'text': 'Taylor Swift', 'answer_start': 1567, 'answer_end': 1579},\n",
       " {'text': '119.5 million', 'answer_start': 1881, 'answer_end': 1894},\n",
       " {'text': 'April 4, 2008', 'answer_start': 3, 'answer_end': 16},\n",
       " {'text': 'Jay Z.', 'answer_start': 34, 'answer_end': 40},\n",
       " {'text': 'Sasha Fierce', 'answer_start': 156, 'answer_end': 168},\n",
       " {'text': 'Single Ladies', 'answer_start': 605, 'answer_end': 618},\n",
       " {'text': 'Kanye West', 'answer_start': 1611, 'answer_end': 1621},\n",
       " {'text': 'Etta James', 'answer_start': 69, 'answer_end': 79},\n",
       " {'text': 'Phoenix House', 'answer_start': 439, 'answer_end': 452},\n",
       " {'text': 'At Last', 'answer_start': 582, 'answer_end': 589},\n",
       " {'text': 'thriller', 'answer_start': 693, 'answer_end': 701},\n",
       " {'text': 'MTV Movie Award for Best Fight',\n",
       "  'answer_start': 1101,\n",
       "  'answer_end': 1131},\n",
       " {'text': 'Phoenix House', 'answer_start': 439, 'answer_end': 452},\n",
       " {'text': 'Obsessed', 'answer_start': 703, 'answer_end': 711},\n",
       " {'text': 'Sharon Charles', 'answer_start': 724, 'answer_end': 738},\n",
       " {'text': '60 million', 'answer_start': 940, 'answer_end': 950},\n",
       " {'text': 'Etta James', 'answer_start': 69, 'answer_end': 79},\n",
       " {'text': 'Phoenix House', 'answer_start': 439, 'answer_end': 452},\n",
       " {'text': \"the First Couple's first inaugural ball.\",\n",
       "  'answer_start': 594,\n",
       "  'answer_end': 634},\n",
       " {'text': 'Obsessed.', 'answer_start': 703, 'answer_end': 712}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c3fa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AliHa\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "encodings = tokenizer(contexts, questions, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14da017d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"contexts\": contexts, \"answers\": answers, \"questions\": questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8fcf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954180f9f3a044299605ba0cbd9778a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: tokenizer(x['contexts'],x['questions'], padding='max_length', truncation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9383ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['contexts', 'answers', 'questions', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ecef46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "for i in range(len(answers)):\n",
    "    start_positions.append(encodings.char_to_token(i, answers[i][\"answer_start\"]))\n",
    "    end_positions.append(encodings.char_to_token(i, answers[i][\"answer_end\"]-1))\n",
    "    if start_positions[-1] is None:\n",
    "        start_positions[-1] = tokenizer.model_max_length\n",
    "    if end_positions[-1] is None:\n",
    "        end_positions[-1] = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1cab705",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(\"start_positions\",start_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c22afd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(\"end_positions\",end_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932b083f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['contexts', 'answers', 'questions', 'input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc6c4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['contexts', 'answers', 'questions', 'input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e3c9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_testvalid = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, shuffle=True)\n",
    "\n",
    "dataset = datasets.DatasetDict({\n",
    "                                'train': train_testvalid['train'],\n",
    "                                'validation': test_valid['train'],\n",
    "                                'test': test_valid['test']\n",
    "                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57ef572",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_examples = dataset[\"validation\"].remove_columns(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c28b348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['contexts', 'answers', 'questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66f55e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/30 03:49 < 06:53, 0.04 it/s, Epoch 1.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.390177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7876/1921234616.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         compute_metrics=compute_metrics,)\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1858\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1859\u001b[1;33m             return inner_training_loop(\n\u001b[0m\u001b[0;32m   1860\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1861\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2202\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2203\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2205\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\accelerate\\accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2122\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2124\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m             )\n\u001b[1;32m--> 522\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_collator = DefaultDataCollator()\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['validation'],\n",
    "        eval_examples=val_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        #post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics,)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cbb24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
